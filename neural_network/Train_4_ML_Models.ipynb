{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a23acf-88ff-43a3-8771-dbae0ae876f4",
   "metadata": {},
   "source": [
    "# Train four different ML models with PyTorch to test linearity and independence\n",
    "\n",
    "1. Train four different ML algorithums on all CMIP6 Global Climate Model large ensembles (LEs) with >15 members\n",
    "2. Train four different ML algorithums on the multi-model CMIP6 ensemble (n=41) with 1st/2nd/3rd members as training/validation/test members\n",
    "3. Remove one variable at a time from the linear model, trained on the first 75% of the LE members\n",
    "4. Remove one variable at a time from the linear model, trained on the CMIP6 1st members  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c11f52-05dd-4337-bcf0-87b080c774a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-17 09:54:05.834879\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import dask\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def33401-b76d-44d8-89d8-75f373f374f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0); #for reproducability\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6157630-a522-4454-b69a-095b7bade1f6",
   "metadata": {},
   "source": [
    "## Load data and define useful functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f82f6403-fcee-4d26-8f50-d7482c54951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWI-CM-1-1-MR Either SIC or CVDP data missing\n",
      "AWI-ESM-1-1-LR Either SIC or CVDP data missing\n",
      "CAS-ESM2-0 Either SIC or CVDP data missing\n",
      "CanESM5-1 No members with good SIC and CVDP data\n",
      "E3SM-2-0 No members with good SIC and CVDP data\n",
      "FGOALS-f3-L Either SIC or CVDP data missing\n",
      "FGOALS-g3 Either SIC or CVDP data missing\n",
      "GISS-E2-1-G-CC Either SIC or CVDP data missing\n",
      "GISS-E3-G Either SIC or CVDP data missing\n",
      "IITM-ESM Either SIC or CVDP data missing\n",
      "KACE-1-0-G Either SIC or CVDP data missing\n",
      "MCM-UA-1-0 Either SIC or CVDP data missing\n"
     ]
    }
   ],
   "source": [
    "#load useful data such as the GCM names and their DOIs\n",
    "CMIP6_info = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/raw_data/CMIP6_info/'\\\n",
    "    +'CMIP6_modeling_center_members_doi.nc'\n",
    ")\n",
    "\n",
    "#loop through all GCMs and make list of all realizations common to regional\n",
    "#SIC data and all CVDP variables\n",
    "good_GCM_mem = {}\n",
    "for GCM in CMIP6_info['model'].values:\n",
    "    try:\n",
    "        #open the regional SIC file and list the members, all members in file have\n",
    "        #already gone through quality control to check no nan or 0 values\n",
    "        SIC = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "            +f'Regional_SIC_detrended_lowpass_filter_{GCM}_1920_2014.nc')\n",
    "        # SIC = xr.open_dataset(\n",
    "        #     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        #     +f'Regional_SIC_bandpass_2_20_year_{GCM}_1920_2014.nc')\n",
    "        SIC_mems = SIC['member'].values\n",
    "\n",
    "        #open the CVDP data and list all of the members which do not have nan values\n",
    "        #for the AMO - if they do have nan values do not use list that member.\n",
    "        CVDP = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "            +f'CVDP_standardized_linear_detrended_1920_2014_historical_{GCM}.nc')\n",
    "        CVDP_mems = CVDP['member'].where(\n",
    "            ~xr.ufuncs.isnan(CVDP['AMO']).max('time'), drop=True)\n",
    "\n",
    "        #now list the members with both good SIC and CVDP data, if at least 1\n",
    "        if len(np.intersect1d(CVDP_mems, SIC_mems)) > 0:\n",
    "            good_GCM_mem[GCM] = np.sort(np.intersect1d(CVDP_mems, SIC_mems))\n",
    "        else:\n",
    "            print(GCM, 'No members with good SIC and CVDP data')\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(GCM, 'Either SIC or CVDP data missing')\n",
    "    \n",
    "#remove CESM2 from the dictionary as replace with CESM2-LENS data\n",
    "good_GCM_mem.pop('CESM2', None);\n",
    "\n",
    "#save the member data\n",
    "# with open('/glade/work/cwpowell/low-frequency-variability/raw_data/CMIP6_info/'\\\n",
    "#           +'CMIP6_members_CVDP_and_SIC.pickle', 'wb') as handle:\n",
    "#     pickle.dump(good_GCM_mem, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7649a79-ea17-4518-9692-f595f2ce6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the train/validation/test split for the large ensembles with at least 15\n",
    "#members\n",
    "train_valid_test = [0.75, 0.15, 0.10]\n",
    "\n",
    "LE_train_mem = {}\n",
    "LE_valid_mem = {}\n",
    "LE_test_mem  = {}\n",
    "LE_GCM_list = []\n",
    "for GCM in np.sort(list(good_GCM_mem.keys())):\n",
    "    \n",
    "    n_mem = len(good_GCM_mem[GCM])\n",
    "    \n",
    "    if n_mem > 15:\n",
    "        LE_GCM_list.append(GCM)\n",
    "        train_n = int(np.ceil(n_mem*train_valid_test[0]))\n",
    "        test_n  = int(np.floor(n_mem*train_valid_test[2]))\n",
    "\n",
    "        LE_train_mem[GCM] = good_GCM_mem[GCM][:train_n]\n",
    "        LE_valid_mem[GCM] = good_GCM_mem[GCM][train_n:-test_n]\n",
    "        LE_test_mem[GCM]  = good_GCM_mem[GCM][-test_n:]\n",
    "        \n",
    "LE_GCM_list = np.sort(LE_GCM_list)\n",
    "\n",
    "#add in the CMIP6 member number codes\n",
    "CVDP_CMIP6_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'CVDP_standardized_linear_detrended_1970_2014_historical_CMIP6.nc')\n",
    "\n",
    "SIC_CMIP6_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'Regional_SIC_detrended_lowpass_filter_CMIP6_1970_2014.nc')\n",
    "\n",
    "# CVDP_CMIP6_xr = xr.open_dataset(\n",
    "#     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "#     'CVDP_standardized_highpass_filt_40_yr_1920_2014_historical_CMIP6.nc')\n",
    "\n",
    "# SIC_CMIP6_xr = xr.open_dataset(\n",
    "#     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "#     'Regional_SIC_bandpass_2_40_year_CMIP6_1920_2014.nc')\n",
    "\n",
    "LE_train_mem['CMIP6'] = CVDP_CMIP6_xr['member'].sel(\n",
    "    member=slice(1000,1999)).values\n",
    "LE_valid_mem['CMIP6'] = CVDP_CMIP6_xr['member'].sel(\n",
    "    member=slice(2000,2999)).values\n",
    "LE_test_mem['CMIP6']  = CVDP_CMIP6_xr['member'].sel(\n",
    "    member=slice(10000,None)).values\n",
    "\n",
    "#add in the CMIP6 member number codes for 30+ members\n",
    "CVDP_CMIP6_30_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'CVDP_standardized_linear_detrended_1920_2014_historical_CMIP6_30.nc')\n",
    "\n",
    "SIC_CMIP6_30_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'Regional_SIC_detrended_lowpass_filter_CMIP6_30_1920_2014.nc')\n",
    "\n",
    "# CVDP_CMIP6_30_xr = xr.open_dataset(\n",
    "#     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "#     +'CVDP_standardized_highpass_filt_40_yr_1920_2014_historical_CMIP6_30.nc')\n",
    "\n",
    "# SIC_CMIP6_30_xr = xr.open_dataset(\n",
    "#     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "#     +'Regional_SIC_bandpass_2_40_year_CMIP6_30_1920_2014.nc')\n",
    "\n",
    "LE_train_mem['CMIP6_30'] = CVDP_CMIP6_30_xr['member'].sel(\n",
    "    member=slice(10000,19999)).values\n",
    "LE_valid_mem['CMIP6_30'] = CVDP_CMIP6_30_xr['member'].sel(\n",
    "    member=slice(20000,29999)).values\n",
    "LE_test_mem['CMIP6_30']  = CVDP_CMIP6_30_xr['member'].sel(\n",
    "    member=slice(30000,None)).values\n",
    "\n",
    "#add in the PI Control member number codes\n",
    "CVDP_PI_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'CVDP_standardized_PI_Control_MMLE_500_first_3_train.nc')\n",
    "\n",
    "SIC_PI_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    'Regional_SIC_lowpass_filter_PI_Control_MMLE_500_first_3_train.nc')\n",
    "\n",
    "LE_train_mem['PI_500'] = CVDP_PI_xr['member'].sel(\n",
    "    member=slice(1000,9999)).values\n",
    "LE_valid_mem['PI_500'] = CVDP_PI_xr['member'].sel(\n",
    "    member=slice(20000,29999)).values\n",
    "LE_test_mem['PI_500']  = CVDP_PI_xr['member'].sel(\n",
    "    member=slice(30000,None)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4037d758-6506-4b03-a117-2c801830e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a list of variable and season names\n",
    "CVDP_sample = xr.open_dataset('/glade/work/cwpowell/low-frequency-variability/'\\\n",
    "    +'input_data/CVDP_standardized_linear_detrended_1920_'\\\n",
    "    +'2014_historical_CanESM5.nc')\n",
    "\n",
    "var_month_list = []\n",
    "for i in CVDP_sample.to_array()['variable'].drop_sel(\n",
    "    variable=['AMOC','NINO12','NINO3','NINO4']).values:\n",
    "    for month_num in [1,4,7,10]:\n",
    "        var_month_list.append(str(i)+'_'+str(month_num))\n",
    "        \n",
    "var_month_list.append('RAND_1')\n",
    "var_month_list.append('RAND_4')\n",
    "var_month_list.append('RAND_7')\n",
    "var_month_list.append('RAND_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c454ace-23d3-42b9-a252-c0f25d8660a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, var_month_name in enumerate(var_month_list):\n",
    "#     print(i,var_month_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00412afa-5e43-4e59-8367-2ff45564c940",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define functions for loading feature and target data, as well as training the 4 ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0243aee8-37df-4099-bddd-5b8974905049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CVDP(model_name, month_, lag_, start_end_yr, extra_drop=None, \n",
    "              white_noise=None):\n",
    "    '''\n",
    "    Load the training, validation and testing data of the climate modes of\n",
    "    varaibility (features), corresponding to the time period of the sea ice\n",
    "    data (target) and the lag time. Additionally, remove certain climate modes\n",
    "    and/or include a white noise variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The month of the target sea ice concentration, this will determine\n",
    "        how many years lagged each season is, e.g. if sea ice concentration\n",
    "        is for October the seasonal CVDP data will be lagged 2 years for DJF,\n",
    "        MAM and JJA, but 3 years for SON.\n",
    "    lag_: int,\n",
    "        The number of years the CVDP data is offset before the sea ice data \n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (insclusive) for the sea ice concentration data.\n",
    "    extra_drop: none or list of strings,\n",
    "        If False, no additonal variables are dropped. If a list of a string or\n",
    "        strings, those variables listed will be removed from the CVDP data. \n",
    "    white_noise: bool,\n",
    "        If True, add a variable of normalized random values in a gaussian \n",
    "        distribution for the 4 seasons, all members and all years.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    CVDP_train: PyTorch tensor,\n",
    "        The stacked CVDP for the training members, with shape \n",
    "        ([member x year],[variable x month]) e.g for the first 75% of the 65\n",
    "        CanESM5 members used for training: ([49x74],[17x4]) = (3626,68).\n",
    "    CVDP_valid: PyTorch tensor,\n",
    "        The stacked CVDP for the validation members, with shape\n",
    "        ([member x year],[variable x month]).\n",
    "    CVDP_test: PyTorch tensor,\n",
    "        The stacked CVDP for the testing members, with shape\n",
    "        ([member x year],[variable x month]).    \n",
    "    ''' \n",
    "    \n",
    "    #load CVDP features and convert to seasonal data\n",
    "    #select the NetCDF file with lowpass filtering or linear detrending\n",
    "    # CVDP_year_month = xr.open_dataset(\n",
    "    #     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    #     +'CVDP_standardized_linear_detrended_1920_2014_historical_'\\\n",
    "    #     +f'{model_name}.nc'\n",
    "    # )\n",
    "    ############ !!!!!!!! COMMENT ABOVE/BELOW FOR PI OR HIST !!!!!!! ###########\n",
    "    CVDP_year_month = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +'CVDP_standardized_PI_Control_MMLE_500_first_3_train.nc'\n",
    "    )\n",
    "\n",
    "    # CVDP_year_month = xr.open_dataset(\n",
    "    #     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    #     +'CVDP_standardized_highpass_filt_40_yr_1920_2014_historical_'\\\n",
    "    #     +f'{model_name}.nc'\n",
    "    # )    \n",
    "        \n",
    "        \n",
    "    CVDP_year_month = CVDP_year_month.to_array('variable').sortby('time')\n",
    "\n",
    "    month_seperate = []\n",
    "    for i in [1,4,7,10]:\n",
    "        temp_data = CVDP_year_month.sel(\n",
    "            time=CVDP_year_month['time.month']==i)\n",
    "        temp_data['time'] = np.arange(1920,2015)\n",
    "        # temp_data['time'] = np.arange(1970,2015)\n",
    "        month_seperate.append(temp_data)\n",
    "\n",
    "    CVDP_data = xr.concat((month_seperate), dim='month')\n",
    "    CVDP_data['month'] = [1,4,7,10]\n",
    "    CVDP_data = CVDP_data.rename({'time':'year'})\n",
    "\n",
    "    ######### !!!!!!!!!!!! UNCOMMENT FOR NON PI CONTROL !!!!!!!!!!!! ##########\n",
    "#     CVDP_data = CVDP_data.drop_sel(variable=['AMOC','NINO12','NINO3','NINO4'])\n",
    "    \n",
    "#     if type(extra_drop) != type(None): #drop extra variables from the CVDP data\n",
    "#         CVDP_data = CVDP_data.drop_sel(variable=extra_drop)\n",
    "    #########  PI CONTROL DOES NOT NEED THIS AS ALREADY NOT INCLUDED  ##########\n",
    "    \n",
    "    #now stack the CVDP data into X members and years, Y features\n",
    "    CVDP_train = []\n",
    "    CVDP_test  = []\n",
    "    CVDP_valid = []\n",
    "    for lag_season in [1,4,7,10]: \n",
    "        if lag_season >= month_:\n",
    "            extra_year = 1\n",
    "        else:\n",
    "            extra_year = 0\n",
    "\n",
    "        CVDP_month_data = CVDP_data.sortby('member')\n",
    "            \n",
    "        CVDP_month_data = CVDP_month_data.sel(\n",
    "            month=lag_season).sel(\n",
    "            year=slice(str(start_end_yr[0]-lag_-extra_year), \n",
    "                       str(start_end_yr[1]-lag_-extra_year)))\n",
    "        CVDP_month_data['year'] = np.arange(\n",
    "            0,start_end_yr[1]-start_end_yr[0]+1)\n",
    "        \n",
    "        #now, optionally add in white noise as 4 seasons of a new variable\n",
    "        if white_noise:\n",
    "            white_noise_month = (\n",
    "                CVDP_month_data.copy().isel(variable=0) * 0 + np.random.normal(\n",
    "                    loc=0, scale=1, size=(len(CVDP_month_data['member']),\n",
    "                                          len(CVDP_month_data['year'])))\n",
    "            )\n",
    "\n",
    "            white_noise_month['variable'] = 'RAND'\n",
    "            \n",
    "            CVDP_month_data = xr.concat(\n",
    "                (CVDP_month_data, white_noise_month), dim='variable'\n",
    "            ) \n",
    "        \n",
    "        CVDP_train.append(CVDP_month_data.sel(member=LE_train_mem[model_name]))\n",
    "        CVDP_valid.append(CVDP_month_data.sel(member=LE_valid_mem[model_name]))\n",
    "        CVDP_test.append(CVDP_month_data.sel(member=LE_test_mem[model_name]))\n",
    "        \n",
    "    CVDP_train_stacked = xr.concat((CVDP_train),'month').stack(\n",
    "        member_time=('member','year')).stack(\n",
    "        var_month=('variable','month'))\n",
    "    CVDP_train = torch.Tensor(CVDP_train_stacked.values)\n",
    "    \n",
    "    CVDP_valid_stacked = xr.concat((CVDP_valid),'month').stack(\n",
    "        member_time=('member','year')).stack(\n",
    "        var_month=('variable','month'))\n",
    "    CVDP_valid = torch.Tensor(CVDP_valid_stacked.values)\n",
    "\n",
    "    CVDP_test_stacked = xr.concat((CVDP_test),'month').stack(\n",
    "        member_time=('member','year')).stack(\n",
    "        var_month=('variable','month'))\n",
    "    CVDP_test = torch.Tensor(CVDP_test_stacked.values)\n",
    "    \n",
    "    return(CVDP_train, CVDP_valid, CVDP_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5d28b73-ac61-454b-8534-9d04c33741a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_SIC(model_name, month_, region_, start_end_yr, \n",
    "             ensemble_detrend=None\n",
    "            ):\n",
    "    '''\n",
    "    Load the sea ice concentration anomalies (targets) for a specific GCM,\n",
    "    month and time period.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The month of the target sea ice concentration anomalies. \n",
    "    region_: int,\n",
    "        The region number for the sea ice concentration anomalies.\n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (inclusive) for the sea ice concentration \n",
    "        anomalies.\n",
    "    ensemble_detrend: None, bool,\n",
    "        Whether to use the lowpass filtered data (when keyword is None or False)\n",
    "        or the ensemble mean for when the keyword is True.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    target_train: PyTorch tensor,\n",
    "        The stacked sea ice concentration anomalies for the training members,\n",
    "        with shape (member, year) e.g for the first 75% of the 65 CanESM5\n",
    "        members used for training: (49,74). \n",
    "    target_valid: PyTorch tensor,\n",
    "        The stacked sea ice concentration anomalies for the validation members,\n",
    "        with shape (member, year).\n",
    "    target_test: PyTorch tensor,\n",
    "        The stacked sea ice concentration anomalies for the test members, with\n",
    "        shape (member, year).\n",
    "    \n",
    "    '''\n",
    "    #load SIC targets with the desired type of filtering/dretrending\n",
    "    if ensemble_detrend:\n",
    "        SIC_data = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "            'Regional_SIC_bandpass_2_40_year_'\\\n",
    "            f'{model_name}_1920_2014.nc'\n",
    "        )\n",
    "    else:\n",
    "        SIC_data = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "            'Regional_SIC_lowpass_filter_PI_Control_MMLE_500_first_3_train.nc')\n",
    "        # SIC_data = xr.open_dataset(\n",
    "        #     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        #     +f'Regional_SIC_detrended_lowpass_filter_{model_name}_1920_2014.nc'\n",
    "        # )\n",
    "        \n",
    "    #select the years for this analysis period and sort by member\n",
    "    SIC_data = SIC_data['SIC'].sortby('member').sel(\n",
    "        year=slice(str(start_end_yr[0]), str(start_end_yr[1])))\n",
    "   \n",
    "    #convert 2D xr.DataArray to 1D xr.DataArray to np.ndarray to torch.Tensor\n",
    "    target_train = torch.from_numpy(\n",
    "        SIC_data.sel(member=LE_train_mem[model_name]).sel(month=month_).sel(\n",
    "            region=region_).stack(member_time=('member','year')).values\n",
    "    )\n",
    "    target_valid = torch.from_numpy(\n",
    "        SIC_data.sel(member=LE_valid_mem[model_name]).sel(month=month_).sel(\n",
    "            region=region_).stack(member_time=('member','year')).values\n",
    "    )\n",
    "    target_test = torch.from_numpy(\n",
    "        SIC_data.sel(member=LE_test_mem[model_name]).sel(month=month_).sel(\n",
    "            region=region_).stack(member_time=('member','year')).values\n",
    "    )\n",
    "    \n",
    "    return(target_train, target_valid, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bd36884-be9c-45f9-af65-81eb299a2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_4_ML_for_LE(model_name, month_, region_list, lag_list, start_end_yr, \n",
    "                      n_epoch, learn_rates, white_noise_=None,\n",
    "                      ens_bool=None):\n",
    "    '''\n",
    "    Train the 4 machine learning models for a given large ensemble on a \n",
    "    specificed of sea ice concentration data for a specified set of months,\n",
    "    regions, and lags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The months of sea ice concentration anomalies on which to train the \n",
    "        machine learning model. \n",
    "    region_list: list of ints,\n",
    "        A list of the regional sea ice anomalies to train the model on \n",
    "        separately.\n",
    "    lag_list: list of ints,\n",
    "        The range of lagged year on which to separately train the machine \n",
    "        learning model \n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (inclusive) for the sea ice concentration data.\n",
    "    n_epoch: int, \n",
    "        number of epochs with which to train all of the machine learning \n",
    "        models.\n",
    "    learn_rates: list of ints,\n",
    "        List of 4 integers corresponding to the learning rate for each of the 4\n",
    "        machine learning algorithums.     \n",
    "    white_noise: bool,\n",
    "        If True, add a variable of normalized random values in a gaussian \n",
    "        distribution for the 4 seasons, all members and all years.\n",
    "    ens_bool: None, bool,\n",
    "        Whether to use the lowpass filtered sea ice concentration data (when \n",
    "        keyword is None or False) or the ensemble mean when the keyword is True.\n",
    "                      \n",
    "    Returns\n",
    "    ----------\n",
    "    r_values_xr: xarray.Dataset,\n",
    "        The pearson correlation coefficients for the 4 machine learning \n",
    "        algorithms from the validation data.\n",
    "    all_1_grads_xr: xarray.Dataset,\n",
    "        The gradients from the machine learning model using a simple multiple\n",
    "        linear regression model.    \n",
    "    '''\n",
    "\n",
    "    if white_noise_:\n",
    "        n_features = 15*4\n",
    "    else:\n",
    "        # n_features = 14*4 #for using 4 seasons for all variables\n",
    "        # n_features = 50 #for using annual value of NPI and IPO\n",
    "        n_features = 36#33 #for removing SAM, IOD, NPI, NAM, AMM\n",
    "        \n",
    "    if model_name in ['CMIP6', 'CMIP6_30', 'PI_500']:\n",
    "        doi_model = '10.5194/gmd-9-1937-2016'\n",
    "    else:\n",
    "        doi_model = CMIP6_info['doi'].sel(model=model_name).values\n",
    "    \n",
    "    all_1_weights = np.empty(\n",
    "        [len(region_list), len(lag_list), n_features], dtype=float)\n",
    "\n",
    "    all_r_values = np.empty(\n",
    "        [len(region_list), len(lag_list), 4], dtype=float)\n",
    "\n",
    "    for region_i, region_ in enumerate(region_list):\n",
    "\n",
    "        for lag_i, lag_ in enumerate(lag_list):\n",
    "            #load the feature and target data for the correct model, month,\n",
    "            #region and lag\n",
    "            CVDP_train, CVDP_valid, CVDP_test = load_CVDP(\n",
    "                model_name, month_, lag_, start_end_yr, \n",
    "                extra_drop=None, white_noise=white_noise_\n",
    "            )\n",
    "            # '''This block removes the MAM, JJA, SON seasons of NPI and IPO'''\n",
    "            # CVDP_train = torch.cat(\n",
    "            #     (CVDP_train[:,0:5], CVDP_train[:,8:29], CVDP_train[:,32:]),\n",
    "            #     dim=1)\n",
    "            # CVDP_valid = torch.cat(\n",
    "            #     (CVDP_valid[:,0:5], CVDP_valid[:,8:29], CVDP_valid[:,32:]),\n",
    "            #     dim=1)\n",
    "            # '''This block removes the MAM, JJA, SON seasons of NPI and IPO'''\n",
    "            \n",
    "            '''Remove NAM, SAM, IOD, NPI, AMM and non DJF IPO values'''\n",
    "            #NOTE CVDP_train[:,0:5], CVDP_train[:,8:16] for just DJF IPO\n",
    "            #OR just CVDP_train[:,0:16] for all IPO seasons\n",
    "            #Below is for the non PI-Control data\n",
    "#             CVDP_train = torch.cat(\n",
    "#                 (CVDP_train[:,0:16], CVDP_train[:,20:24], \n",
    "#                  CVDP_train[:,36:48], CVDP_train[:,52:]), dim=1\n",
    "#             )\n",
    "            \n",
    "#             CVDP_valid = torch.cat(\n",
    "#                 (CVDP_valid[:,0:16], CVDP_valid[:,20:24],\n",
    "#                  CVDP_valid[:,36:48], CVDP_valid[:,52:]), dim=1\n",
    "#             )\n",
    "            \n",
    "            #these 2 lines are for the PI control data which was reduced already\n",
    "            # CVDP_train = torch.cat((CVDP_train[:,0:5], CVDP_train[:,8:]), dim=1)\n",
    "            # CVDP_valid = torch.cat((CVDP_valid[:,0:5], CVDP_valid[:,8:]), dim=1)\n",
    "\n",
    "            '''Remove NAM, SAM, IOD, NPI, AMM and non DJF IPO values'''\n",
    "            \n",
    "#             CVDP_train = torch.index_select(\n",
    "#                 CVDP_train,1, torch.LongTensor(\n",
    "#                     [3,7,10,15,19,20,24,28,34,38,42,47,48,55]))\n",
    "            \n",
    "#             CVDP_valid = torch.index_select(\n",
    "#                 CVDP_valid,1, torch.LongTensor(\n",
    "#                     [3,7,10,15,19,20,24,28,34,38,42,47,48,55]))            \n",
    "            \n",
    "\n",
    "            target_train, target_valid, target_test = load_SIC(\n",
    "                model_name, month_, region_, start_end_yr, \n",
    "                ensemble_detrend=ens_bool,\n",
    "            )\n",
    "\n",
    "            ML_model_computed = []\n",
    "            for ML_i in range(4):\n",
    "                if ML_i == 0:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,1,bias=False)\n",
    "                    )\n",
    "                elif ML_i == 1:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,1,bias=False), nn.ReLU()\n",
    "                    )\n",
    "                elif ML_i == 2:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,n_neurons), \n",
    "                        nn.Linear(n_neurons,n_neurons),\n",
    "                        nn.Linear(n_neurons,1)\n",
    "                    )\n",
    "                    \n",
    "                elif ML_i == 3:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,n_neurons), nn.ReLU(),\n",
    "                        nn.Linear(n_neurons,n_neurons), nn.ReLU(),\n",
    "                        nn.Linear(n_neurons,1)\n",
    "                    )\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    params=ML_model_use.parameters(), \n",
    "                    lr=learn_rates[ML_i]\n",
    "                )\n",
    "\n",
    "                ##### train the model #####\n",
    "                train_r, valid_r, valid_loss = [],[],[]\n",
    "                for epoch in range(n_epoch):\n",
    "                    # TRAIN\n",
    "                    prediction = ML_model_use(CVDP_train)\n",
    "                    optimizer.zero_grad() #reset the gradients to zeros\n",
    "                    loss = loss_fcn(prediction[:,0].double(), target_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_r.append(\n",
    "                        np.corrcoef(prediction[:,0].detach().numpy(),\n",
    "                                    target_train.detach().numpy()\n",
    "                                   )[1][0]\n",
    "                    )\n",
    "\n",
    "                    if ML_i == 0:\n",
    "                        ML_model_computed.append(ML_model_use)\n",
    "\n",
    "                    ##### validate the model #####\n",
    "                    with torch.no_grad():\n",
    "                        p_val = ML_model_use(CVDP_valid)\n",
    "                        loss_val = loss_fcn(p_val[:,0].double(), \n",
    "                                            target_valid\n",
    "                                           )\n",
    "                        valid_loss.append(loss_val)\n",
    "                        \n",
    "                        valid_r.append(\n",
    "                            np.corrcoef(p_val[:,0].detach().numpy(),\n",
    "                                        target_valid.detach().numpy()\n",
    "                                       )[1][0]\n",
    "                        )\n",
    "\n",
    "                #select highest validation r value from all epochs\n",
    "                # all_r_values[region_i][lag_i][ML_i] = np.max(valid_r)\n",
    "                all_r_values[region_i][lag_i][ML_i] = valid_r[np.argmin(\n",
    "                    valid_loss)]\n",
    "\n",
    "                if ML_i == 0: #record the weights of the best epoch\n",
    "                    # all_1_weights[region_i][lag_i] = np.ravel(\n",
    "                    #     ML_model_computed[np.argmax(valid_r)][\n",
    "                    #         0].weight[0,:].detach().numpy())\n",
    "                    all_1_weights[region_i][lag_i] = np.ravel(\n",
    "                        ML_model_computed[np.argmin(valid_loss)][\n",
    "                            0].weight[0,:].detach().numpy())\n",
    "\n",
    "    if model_name == 'CMIP6':\n",
    "        model_name = 'CMIP6 multi-model large ensemble'\n",
    "    \n",
    "    #save the r values to NetCDF every model and month\n",
    "    r_values_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'r_value':(['region', 'lag', 'ML_model'], all_r_values)},\n",
    "        coords = {\n",
    "            'region':region_list, 'lag':lag_list, 'ML_model':range(4)},\n",
    "    )\n",
    "\n",
    "    r_values_xr_attrs = {\n",
    "        'Description': 'Pearson correlation coefficient for validation '\\\n",
    "            +'data (15%) of the availible members of the global climate '\\\n",
    "            +f'model {model_name}. 4 different machine learning models '\\\n",
    "            +'fit the features of seasonal climate modes computed by '\\\n",
    "            +'the Climate Variability Diagnostics Package with 2 year '\\\n",
    "            +'lowpass filtered sea ice concentration. These climate modes are '\\\n",
    "            +f'as follows: AMO, IPO, NINO34, PDO, ATN, '\\\n",
    "            +'NPO, PNA, NAO, TAS. The ML models are '\\\n",
    "            +'trained at different lag times of 1-20 years and for each '\\\n",
    "            +'region (regions are defined by NSIDC MASIE-NH Version 1 '\\\n",
    "            +'(doi:10.7265/N5GT5K3K). The ML_model dimension refers to '\\\n",
    "            +'the 4 different ML architectures with PyTorch all using L1 '\\\n",
    "            +f'loss function and Adam optimizer and {n_epoch} '\\\n",
    "            +'epochs:'+'\\n'+'1 - nn.Sequential'\\\n",
    "            +f'(nn.Linear({n_features},1,bias=False)), learning rate = '\\\n",
    "            +f'{learn_rates[0]}.'+'\\n'+'2 - nn.Sequential(nn.Linear('\\\n",
    "            +f'{n_features},1,bias=False), nn.ReLU()), learning rate = '\\\n",
    "            +f'{learn_rates[1]}.'+'\\n3 - nn.Sequential(nn.Linear('\\\n",
    "            +f'{n_features},{n_neurons}), nn.Linear({n_neurons},'\\\n",
    "            +f'{n_neurons}), nn.Linear({n_neurons},1)), learning rate = '\\\n",
    "            +f'{learn_rates[2]}.'+'\\n'+'4 - nn.Sequential(nn.Linear('\\\n",
    "            +f'{n_features},{n_neurons}), nn.ReLU(),nn.Linear({n_neurons}'\\\n",
    "            +f',{n_neurons}), nn.ReLU(), nn.Linear({n_neurons},1)), '\\\n",
    "            +f'learning rate {learn_rates[3]}.',\n",
    "        'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "            \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "        'Data source': f'CMIP6 global climate model {model_name}, '\\\n",
    "            +f'doi:{doi_model} sea ice concentration and climate modes '\\\n",
    "            +'calculated by the Climate Variability Diagnostics Package '\\\n",
    "            +'(doi:10.1002/2014EO490002)',\n",
    "        'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "            +'variability/blob/main/neural_network/4_PyTorch_model_'\\\n",
    "            +'configurations.ipynb',\n",
    "    }\n",
    "\n",
    "    r_values_xr.attrs = r_values_xr_attrs\n",
    "\n",
    "    #save the linear 1 layer neural network weights to NetCDF\n",
    "    if white_noise_:\n",
    "        var_month_use = np.array(var_month_list).copy()\n",
    "    else:\n",
    "        # var_month_use = np.array(var_month_list[:-4]).copy()\n",
    "        '''REMOVE THIS BLOCK IF WANT MORE THAN 1 VALUE NPI, IPO'''\n",
    "        # var_month_use = np.delete(np.array(var_month_list[:-4]).copy(),[5,6,7,29,30,31])\n",
    "        # var_month_use = ['AMO','IPO','NINO34','PDO','AMM','ATN','IOD','NPI',\n",
    "        #                  'NAM','NPO','PNA','NAO','SAM','TAS']\n",
    "        '''REMOVE THIS BLOCK IF WANT MORE THAN 1 VALUE NPI, IPO'''\n",
    "        #use line below only if want 9 vars with single DJF IPO value\n",
    "        # var_month_use = np.delete(np.array(var_month_list[:-4]).copy(),\n",
    "        #     [5,6,7,16,17,18,19,24,25,26,27,28,29,30,31,32,33,34,35,48,49,50,51])\n",
    "        \n",
    "        #use line below if using 9 vars with ALL IPO values\n",
    "        var_month_use = np.delete(np.array(var_month_list[:-4]).copy(),\n",
    "            [16,17,18,19,24,25,26,27,28,29,30,31,32,33,34,35,48,49,50,51])\n",
    "        \n",
    "    all_1_weights_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'weights':(['region', 'lag', 'mode_month'], all_1_weights),\n",
    "        },\n",
    "        coords = {'region':region_list, 'lag':lag_list,\n",
    "                  'mode_month':var_month_use,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    all_1_weights_xr_attrs = r_values_xr_attrs.copy()\n",
    "    all_1_weights_xr_attrs['Description'] = 'Weights of the '\\\n",
    "        +'linear model fit for the validation data (15%) of the availible '\\\n",
    "        +f'members of the global climate model {model_name}. {n_features} '\\\n",
    "        +'features of seasonal climate modes computed by the Climate '\\\n",
    "        +'Variability Diagnostics Package with 2 year lowpass filtered sea '\\\n",
    "        +'ice concentration. These climate modes are as follows: AMO, IPO, '\\\n",
    "        +'NINO34, PDO, ATN, NPO, PNA, NAO, TAS. '\\\n",
    "        +'The model is trained at different lag times '\\\n",
    "        +'of 1-20 years and for each region (regions are defined by NSIDC '\\\n",
    "        +'MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). The model uses '\\\n",
    "        +f'PyTorch with a L1 loss function, {n_epoch} epochs, Adam'\\\n",
    "        +f' optimizer and is defined by nn.Linear({n_features},1,'\\\n",
    "        +f'bias=False)), learning rate = {learn_rates[0]}.'\n",
    "\n",
    "    all_1_weights_xr.attrs = all_1_weights_xr_attrs\n",
    "        \n",
    "    return(r_values_xr, all_1_weights_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc478daa-f246-49e8-9cf3-e303512a9a9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Train the 4 ML models with the first 75% of LE members, tested on next 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6af0b15-0351-466f-91df-177ad9d4238c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-17 10:10:19.898648 9\n",
      "2023-10-17 10:10:19.898708 ACCESS-ESM1-5\n",
      "2023-10-17 10:33:58.374795 CESM2-LENS\n",
      "2023-10-17 10:58:37.787520 CNRM-CM6-1\n",
      "2023-10-17 11:21:01.013167 CanESM5\n",
      "2023-10-17 11:47:02.387114 EC-Earth3\n",
      "2023-10-17 12:09:38.576934 GISS-E2-1-G\n",
      "2023-10-17 12:33:38.798835 GISS-E2-1-H\n",
      "2023-10-17 12:56:16.382415 IPSL-CM6A-LR\n",
      "2023-10-17 13:19:33.358826 MIROC-ES2L\n",
      "2023-10-17 14:07:05.955468 MPI-ESM1-2-LR\n",
      "2023-10-17 14:30:02.273150 NorCPM1\n"
     ]
    }
   ],
   "source": [
    "#use this cell for running each of the individual GCM LEs\n",
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "n_neurons = 6 \n",
    "\n",
    "for month__ in [1,2,3,4,5,6,7,8,10,11,12]:\n",
    "    print(datetime.datetime.now(), month__)\n",
    "\n",
    "    for model_name in LE_GCM_list[:-1]:\n",
    "        print(datetime.datetime.now(), model_name)\n",
    "        \n",
    "        r_values_xr, all_1_weights_xr = train_4_ML_for_LE(\n",
    "            model_name, month_=month__, region_list=[1,2,3,4,5,6,11], \n",
    "            lag_list=np.arange(1,21), start_end_yr=[1941,2014], n_epoch=2000, \n",
    "            learn_rates=[5e-4, 5e-4, 1e-4, 2e-4], white_noise_=False,\n",
    "            ens_bool=False,\n",
    "        )    \n",
    "\n",
    "        r_values_xr.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'validation_r_values_4ML_{model_name}_month_'\\\n",
    "            +f'{str(month__).zfill(2)}_var_9_all_IPO_lowpass_filt.nc')\n",
    "        all_1_weights_xr.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'weights_linear_{model_name}_month_{str(month__).zfill(2)}_'\\\n",
    "            +'var_9_all_IPO_lowpass_filt.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00724565-f543-4700-b12c-82c8aeb11501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "# n_neurons = 8\n",
    "\n",
    "# for month__ in [8,9]:\n",
    "#     print(datetime.datetime.now(), month__)\n",
    "\n",
    "#     for model_name in LE_GCM_list:\n",
    "#         print(datetime.datetime.now(), model_name)\n",
    "#         #N.B. changed learn rates, should invetigate further to find best values\n",
    "#         #for now the learn rates are doubled and so is the epoch\n",
    "#         r_values_xr, all_1_weights_xr = train_4_ML_for_LE(\n",
    "#             model_name, month_=month__, region_list=[2,11], \n",
    "#             lag_list=np.arange(1,21), start_end_yr=[1941,2014], n_epoch=4000, \n",
    "#             learn_rates=[1e-3, 1e-3, 2e-4, 4e-4], white_noise_=False,\n",
    "#             ens_bool=True,\n",
    "#         )    \n",
    "\n",
    "#         r_values_xr.to_netcdf(\n",
    "#             '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "#             +f'validation_r_values_4ML_{model_name}_month_'\\\n",
    "#             +f'{str(month__).zfill(2)}_var_14_annual_IPO_NPI_ens_detrend.nc')\n",
    "#         all_1_weights_xr.to_netcdf(\n",
    "#             '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "#             +f'weights_linear_{model_name}_month_{str(month__).zfill(2)}_'\\\n",
    "#             +'var_14_annual_IPO_NPI_ens_detrend.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e82c4-7165-4429-a924-d805e807ccd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Train the 4 ML models with all CMIP6 GCMs, 1st/2nd/3rd+ or 1-7/8-9/10+ members training/validation/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b8062-5819-491a-b4fd-0f54c586ee6b",
   "metadata": {},
   "source": [
    "### Make the CMIP6 CVDP and SIC files as if 'CMIP6' was a GCM name and each GCM member 1 was a different member "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "641fca58-f4ac-4797-ba59-bcaddcdc078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain all of the train/validate/test GCM members\n",
    "CMIP6_GCM_list = []\n",
    "for GCM in np.sort(list(good_GCM_mem.keys())):    \n",
    "    n_mem = len(good_GCM_mem[GCM])\n",
    "    if n_mem > 2:\n",
    "        CMIP6_GCM_list.append(GCM)\n",
    "        \n",
    "#gather all of the members together and save to NetCDF\n",
    "CVDP_CMIP6 = []\n",
    "SIC_CMIP6 = []\n",
    "\n",
    "train_mem_i = 1000\n",
    "valid_mem_i = 2000\n",
    "test_mem_i  = 30000\n",
    "\n",
    "for GCM in CMIP6_GCM_list:\n",
    "    #loop through and append the correct members to the train, validation \n",
    "    #and testing groups with the \n",
    "    CVDP_data = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +'CVDP_standardized_linear_detrended_1920_2014_historical_'\\\n",
    "        +f'{GCM}.nc'\n",
    "    )\n",
    "    # CVDP_data = xr.open_dataset(\n",
    "    #     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    #     +'CVDP_standardized_highpass_filt_40_yr_1920_2014_historical_'\\\n",
    "    #     +f'{GCM}.nc'\n",
    "    # )\n",
    "    \n",
    "    CVDP_data = CVDP_data.sel(member=good_GCM_mem[GCM])\n",
    "    \n",
    "    new_mem_list = np.arange(test_mem_i, test_mem_i+len(CVDP_data['member'])-2)\n",
    "    new_mem_list = np.insert(new_mem_list, 0, [train_mem_i, valid_mem_i])\n",
    "    \n",
    "    CVDP_data['member'] = new_mem_list\n",
    "    CVDP_CMIP6.append(CVDP_data)\n",
    "    \n",
    "    #now do the same for the SIC data\n",
    "    SIC_data = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +f'Regional_SIC_detrended_lowpass_filter_{GCM}_1920_2014.nc'\n",
    "    )\n",
    "    # SIC_data = xr.open_dataset(\n",
    "    #     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    #     +f'Regional_SIC_bandpass_2_40_year_{GCM}_1920_2014.nc'\n",
    "    # )\n",
    "    \n",
    "    SIC_data = SIC_data.sel(member=good_GCM_mem[GCM])\n",
    "    \n",
    "    SIC_data['member'] = new_mem_list   \n",
    "    SIC_CMIP6.append(SIC_data)\n",
    "    \n",
    "    #now increase the initial value of the training, validation, and test member\n",
    "    #element numbers\n",
    "    train_mem_i += 1\n",
    "    valid_mem_i += 1\n",
    "    test_mem_i = test_mem_i + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8653df52-8c18-426e-a15f-de0551ee5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this CMIP6 data to NetCDF and include metadata\n",
    "CVDP_CMIP6_xr = xr.concat((CVDP_CMIP6),dim='member').sortby('member')\n",
    "\n",
    "CVDP_CMIP6_xr.attrs = {\n",
    "    'Description' : '40-year highpass filtered and standardized variables '\\\n",
    "        +'from the CVDP (Climate Variability Diagnostics Package) for all '\\\n",
    "        +'CMIP6 global climate models with at least 3 available members.  '\\\n",
    "        +'Seasonal data for 1920-2014. The members can be decoded as follows: '\\\n",
    "        +'1000-1999 are the training members, 2000-2999 are the validation '\\\n",
    "        +'members, and 10000+ are the test members. The GCM is encoded as '\\\n",
    "        +'the last 2 digits for the training and validation members, and the '\\\n",
    "        +'first 2 digits +10 for the testing members. Note there is always '\\\n",
    "        +'1 member from each GCM for training and validation, and all '\\\n",
    "        +'remaining members are with the testing dataset. The GCM numbering '\\\n",
    "        +'referrs to the following: 0 ACCESS-CM2, 1 ACCESS-ESM1-5, '\\\n",
    "        +'2 BCC-CSM2-MR, 3 BCC-ESM1, 4 CAMS-CSM1-0, 5 CESM2-FV2, '\\\n",
    "        +'6 CESM2-LENS, 7 CESM2-WACCM, 8 CESM2-WACCM-FV2, 9 CIESM, '\\\n",
    "        +'10 CMCC-CM2-SR5, 11 CNRM-CM6-1, 12 CNRM-ESM2-1, 13 CanESM5, '\\\n",
    "        +'14 CanESM5-CanOE, 15 E3SM-1-0, 16 EC-Earth3, 17 EC-Earth3-CC, '\\\n",
    "        +'18 EC-Earth3-Veg, 19 EC-Earth3-Veg-LR, 20 FIO-ESM-2-0, '\\\n",
    "        +'21 GFDL-ESM4, 22 GISS-E2-1-G, 23 GISS-E2-1-H, 24 GISS-E2-2-G, '\\\n",
    "        +'25 GISS-E2-2-H, 26 HadGEM3-GC31-LL, 27 HadGEM3-GC31-MM, '\\\n",
    "        +'28 INM-CM5-0, 29 IPSL-CM6A-LR, 30 MIROC-ES2H, 31 MIROC-ES2L, '\\\n",
    "        +'32 MIROC6, 33 MPI-ESM-1-2-HAM, 34 MPI-ESM1-2-HR, 35 MPI-ESM1-2-LR, '\\\n",
    "        +'36 MRI-ESM2-0, 37 NESM3, 38 NorCPM1, 39 NorESM2-LM, 40 NorESM2-MM, '\\\n",
    "        +'41 UKESM1-0-LL. All members are sorted alphabetically before '\\\n",
    "        +'being divided into the three groups',\n",
    "    'Units' :'standardized values',\n",
    "    'Timestamp' : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': 'CMIP6 historical simulations, computed by CVDP '\\\n",
    "        +'doi: 10.1002/2014EO490002.',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-fequency-variability/'\\\n",
    "            +'neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "    \n",
    "CVDP_CMIP6_xr.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'CVDP_standardized_linear_detrended_1920_2014_historical_CMIP6.nc')\n",
    "\n",
    "SIC_CMIP6_xr = xr.concat((SIC_CMIP6),dim='member').sortby('member')\n",
    "SIC_CMIP6_xr.attrs = {\n",
    "    'Description' : '2 to 40-year bandpass filter of regional '\\\n",
    "        +'average sea ice concentration (SIC) in % for the climate model for '\\\n",
    "        +'all CMIP6 global climate models with at least 3 available members. '\\\n",
    "        +'Seasonal data for 1920-2014. The members can be decoded as follows: '\\\n",
    "        +'1000-1999 are the training members, 2000-2999 are the validation '\\\n",
    "        +'members, and 10000+ are the test members. The GCM is encoded as '\\\n",
    "        +'the last 2 digits for the training and validation members, and the '\\\n",
    "        +'first 2 digits +10 for the testing members. Note there is always '\\\n",
    "        +'1 member from each GCM for training and validation, and all '\\\n",
    "        +'remaining members are with the testing dataset. The GCM numbering '\\\n",
    "        +'referrs to the following: 0 ACCESS-CM2, 1 ACCESS-ESM1-5, '\\\n",
    "        +'2 BCC-CSM2-MR, 3 BCC-ESM1, 4 CAMS-CSM1-0, 5 CESM2-FV2, '\\\n",
    "        +'6 CESM2-LENS, 7 CESM2-WACCM, 8 CESM2-WACCM-FV2, 9 CIESM, '\\\n",
    "        +'10 CMCC-CM2-SR5, 11 CNRM-CM6-1, 12 CNRM-ESM2-1, 13 CanESM5, '\\\n",
    "        +'14 CanESM5-CanOE, 15 E3SM-1-0, 16 EC-Earth3, 17 EC-Earth3-CC, '\\\n",
    "        +'18 EC-Earth3-Veg, 19 EC-Earth3-Veg-LR, 20 FIO-ESM-2-0, '\\\n",
    "        +'21 GFDL-ESM4, 22 GISS-E2-1-G, 23 GISS-E2-1-H, 24 GISS-E2-2-G,'\\\n",
    "        +'25 GISS-E2-2-H, 26 HadGEM3-GC31-LL, 27 HadGEM3-GC31-MM, '\\\n",
    "        +'28 INM-CM5-0, 29 IPSL-CM6A-LR, 30 MIROC-ES2H, 31 MIROC-ES2L, '\\\n",
    "        +'32 MIROC6, 33 MPI-ESM-1-2-HAM, 34 MPI-ESM1-2-HR, 35 MPI-ESM1-2-LR, '\\\n",
    "        +'36 MRI-ESM2-0, 37 NESM3, 38 NorCPM1, 39 NorESM2-LM, 40 NorESM2-MM, '\\\n",
    "        +'41 UKESM1-0-LL. All members are sorted alphabetically before '\\\n",
    "        +'being divided into the three groups',\n",
    "    'Timestamp' : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': 'CMIP6 historical simulations',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-fequency-variability/'\\\n",
    "            +'neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "    \n",
    "SIC_CMIP6_xr.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'Regional_SIC_detrended_lowpass_filter_CMIP6_1920_2014.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301aa5a-84b1-43ee-af08-923aeb363e64",
   "metadata": {},
   "source": [
    "### Now do the same for GCMs with 30+ members, but exclude NorCPM1 as it has poor validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da277c2b-f43c-4e1e-9372-f46ec5ed2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain all of the train/validate/test GCM members\n",
    "CMIP6_GCM_30_list = [\n",
    "    'ACCESS-ESM1-5', 'CESM2-LENS', 'CanESM5', 'GISS-E2-1-G', 'IPSL-CM6A-LR', \n",
    "    'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-LR', 'NorCPM1',\n",
    "]\n",
    "        \n",
    "#gather all of the members together and save to NetCDF\n",
    "CVDP_CMIP6_30 = []\n",
    "SIC_CMIP6_30 = []\n",
    "\n",
    "train_mem_i = 10000\n",
    "valid_mem_i = 20000\n",
    "test_mem_i  = 30000\n",
    "\n",
    "for GCM in CMIP6_GCM_30_list:\n",
    "    #loop through and append the correct members to the train, validation \n",
    "    #and testing groups \n",
    "    CVDP_data = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +'CVDP_standardized_linear_detrended_1920_2014_historical_'\\\n",
    "        +f'{GCM}.nc'\n",
    "    )\n",
    "    # CVDP_data = xr.open_dataset(\n",
    "    #     '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    #     +'CVDP_standardized_highpass_filt_40_yr_1920_2014_historical_'\\\n",
    "    #     +f'{GCM}.nc'\n",
    "    # )\n",
    "    \n",
    "    CVDP_data = CVDP_data.sel(member=good_GCM_mem[GCM])\n",
    "    \n",
    "    new_mem_list = np.arange(test_mem_i, test_mem_i+len(CVDP_data['member'])-28)\n",
    "    new_mem_list = np.insert(new_mem_list, 0,\n",
    "                             np.arange(valid_mem_i, valid_mem_i+5))\n",
    "    new_mem_list = np.insert(new_mem_list, 0,\n",
    "                             np.arange(train_mem_i, train_mem_i+23))\n",
    "    \n",
    "    CVDP_data['member'] = new_mem_list\n",
    "    CVDP_CMIP6_30.append(CVDP_data)\n",
    "    \n",
    "    #now do the same for the SIC data\n",
    "    SIC_data = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +f'Regional_SIC_detrended_lowpass_filter_{GCM}_1920_2014.nc'\n",
    "    )\n",
    "#     SIC_data = xr.open_dataset(\n",
    "#         '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "#         +f'Regional_SIC_bandpass_2_40_year_{GCM}_1920_2014.nc'\n",
    "#     )\n",
    "    \n",
    "    SIC_data = SIC_data.sel(member=good_GCM_mem[GCM])\n",
    "    \n",
    "    SIC_data['member'] = new_mem_list   \n",
    "    SIC_CMIP6_30.append(SIC_data)\n",
    "    \n",
    "    #now increase the initial value of the training, validation, and test member\n",
    "    #element numbers\n",
    "    train_mem_i += 1000\n",
    "    valid_mem_i += 1000\n",
    "    test_mem_i  += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "400f1ae8-6185-4fee-9e39-a04465679b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this CMIP6 data to NetCDF and include metadata\n",
    "CVDP_CMIP6_30_xr = xr.concat((CVDP_CMIP6_30),dim='member').sortby('member')\n",
    "\n",
    "CVDP_CMIP6_30_xr.attrs = {\n",
    "    'Description' : '40-year highpass filtered and standardized variables '\\\n",
    "        +'from the CVDP (Climate Variability Diagnostics Package) for all '\\\n",
    "        +'CMIP6 global climate models with at least 30 available members. '\\\n",
    "        +'Seasonal data for 1920-2014. The members can be decoded as follows: '\\\n",
    "        +'10000-19999 are the training members, 20000-29999 are the '\\\n",
    "        +'validation members, and 30000+ are the test members. The member '\\\n",
    "        +'number is encoded as the last 2 digits, with the second digit '\\\n",
    "        +'indicating the GCM as follows, 0:ACCESS-ESM1-5, 1:CESM2-LENS, '\\\n",
    "        +'2:CanESM5, 3:GISS-E2-1-G, 4:IPSL-CM6A-LR, 5:MIROC-ES2L, 6:MIROC6, '\\\n",
    "        +'7:MPI-ESM1-2-LR, 8:NorCPM1. All members are sorted alphabetically '\\\n",
    "        +'before being divided into the three groups.',\n",
    "    'Units' :'standardized values',\n",
    "    'Timestamp' : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': 'CMIP6 historical simulations, computed by CVDP '\\\n",
    "        +'doi: 10.1002/2014EO490002.',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-fequency-variability/'\\\n",
    "            +'neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "    \n",
    "CVDP_CMIP6_30_xr.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'CVDP_standardized_highpass_filt_40_yr_1920_2014_historical_CMIP6'\\\n",
    "    +'_30.nc')\n",
    "\n",
    "SIC_CMIP6_30_xr = xr.concat((SIC_CMIP6_30),dim='member').sortby('member')\n",
    "SIC_CMIP6_30_xr.attrs = {\n",
    "    'Description' : '2 to 40  year bandpass of regional '\\\n",
    "        +'average sea ice concentration (SIC) in % for the climate model for '\\\n",
    "        +'all CMIP6 global climate models with at least 30 available members. '\\\n",
    "        +'Seasonal data for 1920-2014. The members can be decoded as follows: '\\\n",
    "        +'10000-19999 are the training members, 20000-29999 are the '\\\n",
    "        +'validation members, and 30000+ are the test members. The member '\\\n",
    "        +'number is encoded as the last 2 digits, with the second digit '\\\n",
    "        +'indicating the GCM as follows, 0:ACCESS-ESM1-5, 1:CESM2-LENS, '\\\n",
    "        +'2:CanESM5, 3:GISS-E2-1-G, 4:IPSL-CM6A-LR, 5:MIROC-ES2L, 6:MIROC6, '\\\n",
    "        +'7:MPI-ESM1-2-LR, 8:NorCPM1. All members are sorted alphabetically '\\\n",
    "        +'before being divided into the three groups.',\n",
    "    'Timestamp' : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': 'CMIP6 historical simulations',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-fequency-variability/'\\\n",
    "            +'neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "    \n",
    "SIC_CMIP6_30_xr.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'Regional_SIC_bandpass_2_40_year_CMIP6_30_1920_2014.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03506a0-d1d0-436d-9813-497973201f5f",
   "metadata": {},
   "source": [
    "## Now train the 4 ML models on the CMIP6 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c27196fa-dd96-49c8-b5da-c738b599734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-18 12:21:29.372173 9 MMLE 3+\n"
     ]
    }
   ],
   "source": [
    "#firstly using all 1st members for all GCMs with 3+ members\n",
    "#N.B. need to rerun LE_train/valid/test_mem values near top for 1970-2014\n",
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "n_neurons = 6\n",
    "\n",
    "for month__ in [9]:#np.arange(1,13):\n",
    "    print(datetime.datetime.now(), month__, 'MMLE 3+')\n",
    "\n",
    "    r_values_xr, all_1_weights_xr = train_4_ML_for_LE(\n",
    "        'PI_500', month_=month__, region_list=[1,2,3,4,5,6,11], \n",
    "        lag_list=np.arange(1,21), start_end_yr=[1991,2014], n_epoch=2000, \n",
    "        learn_rates=[5e-4, 5e-4, 1e-4, 2e-4], white_noise_=False, \n",
    "        ens_bool=False,\n",
    "    )    \n",
    "\n",
    "    r_values_xr.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        'validation_r_values_4ML_PI_500_first_3_mem_train_month_'\\\n",
    "        f'{str(month__).zfill(2)}_var_9_all_IPO_lowpass_filt_74_year_mem.nc')\n",
    "    all_1_weights_xr.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        'weights_linear_PI_500_first_3_mem_train_month_'\\\n",
    "        f'{str(month__).zfill(2)}_var_9_all_IPO_lowpass_filt_74_year_mem.nc')\n",
    "    \n",
    "    # r_values_xr.to_netcdf(\n",
    "    #     '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    #     'validation_r_values_4ML_CMIP6_month_'\\\n",
    "    #     f'{str(month__).zfill(2)}_var_9_all_IPO_lowpass_filt_1970_2014.nc')\n",
    "    # all_1_weights_xr.to_netcdf(\n",
    "    #     '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    #     'weights_linear_CMIP6_month_'\\\n",
    "    #     f'{str(month__).zfill(2)}_var_9_all_IPO_lowpass_filt_1970_2014.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1d480-15ae-4149-b4c4-61595c63613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#secondly for all 1st-23rd members for all GCMs with 30+ members\n",
    "#note increased the number of epochs to 4000 from 2000 for the smaller dataset\n",
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "n_neurons = 6\n",
    "\n",
    "for month__ in [8,9,10,11,12,1,2,3,4,5,6,7]:\n",
    "    print(datetime.datetime.now(), month__)\n",
    "\n",
    "    r_values_xr, all_1_weights_xr = train_4_ML_for_LE(\n",
    "        'CMIP6_30', month_=month__, region_list=[1,2,3,4,5,6,11], \n",
    "        lag_list=np.arange(1,21), start_end_yr=[1941,2014], n_epoch=2000, \n",
    "        learn_rates=[5e-4, 5e-4, 1e-4, 2e-4], white_noise_=False, ens_bool=False,\n",
    "    )    \n",
    "\n",
    "    r_values_xr.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_4ML_CMIP6_30_month_{str(month__).zfill(2)}_'\\\n",
    "        +f'var_9_all_IPO_lowpass_filt.nc')\n",
    "    all_1_weights_xr.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_CMIP6_30_month_{str(month__).zfill(2)}_'\\\n",
    "        +'var_9_all_IPO_lowpass_filt.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cd28e-3c11-4c67-9d84-ce73762404d0",
   "metadata": {},
   "source": [
    "## Test the CMIP6-trained linear model with 3rd members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcf128d9-7bb3-4f18-9286-cff19fa8c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weight data \n",
    "CMIP6_weights = []\n",
    "for month_ in np.arange(1,13):\n",
    "    all_1_weights_xr = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_CMIP6_month_{str(month_).zfill(2)}_'\\\n",
    "        +'var_9_all_IPO_lowpass_filt.nc'\n",
    "    )\n",
    "    CMIP6_weights.append(all_1_weights_xr['weights'])\n",
    "\n",
    "CMIP6_weights = xr.concat((CMIP6_weights), dim='month')\n",
    "CMIP6_weights['month'] = np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "044c1a09-26d5-458f-bd73-85b4d285f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-18 09:52:28.814195 1\n",
      "2023-10-18 09:52:44.578743 2\n",
      "2023-10-18 09:53:00.811190 3\n",
      "2023-10-18 09:53:17.174374 4\n",
      "2023-10-18 09:53:33.406937 5\n",
      "2023-10-18 09:53:49.687625 6\n",
      "2023-10-18 09:54:05.995534 7\n",
      "2023-10-18 09:54:22.277564 8\n",
      "2023-10-18 09:54:38.816618 9\n",
      "2023-10-18 09:54:55.091383 10\n",
      "2023-10-18 09:55:11.279469 11\n",
      "2023-10-18 09:55:27.298995 12\n"
     ]
    }
   ],
   "source": [
    "month_r_vals = []\n",
    "for month_ in np.arange(1,13):\n",
    "    print(datetime.datetime.now(), month_)\n",
    "    lag_r_vals = []\n",
    "    for lag_ in np.arange(1,21):\n",
    "\n",
    "        CVDP_train, CVDP_valid, CVDP_test = load_CVDP(\n",
    "            'CMIP6', month_, lag_, [1941,2014], \n",
    "            extra_drop=None, white_noise=False,\n",
    "        )\n",
    "        \n",
    "        # CVDP_test = torch.cat(\n",
    "        #         (CVDP_test[:,0:5], CVDP_test[:,8:29], CVDP_test[:,32:]),\n",
    "        #         dim=1)\n",
    "        \n",
    "        CVDP_test = torch.cat(\n",
    "                (CVDP_test[:,0:16], CVDP_test[:,20:24], \n",
    "                 CVDP_test[:,36:48], CVDP_test[:,52:]), dim=1\n",
    "            )\n",
    "            \n",
    "        \n",
    "        region_r_vals = []\n",
    "        for region_ in [1,2,3,4,5,6,11]:\n",
    "            target_train, target_valid, target_test = load_SIC(\n",
    "                 'CMIP6', month_, region_, [1941,2014], \n",
    "            )\n",
    "            \n",
    "            prediction = np.sum(\n",
    "                np.array(CVDP_test) \n",
    "                * np.tile(CMIP6_weights.sel(month=month_).sel(\n",
    "                    region=region_).sel(lag=lag_), \n",
    "                [38850,1]), axis=1\n",
    "            )\n",
    "            \n",
    "            all_test_mem = []\n",
    "            #loop through the 525 ensemble members to test\n",
    "            for i in range(525):\n",
    "                #increment by 74 years for (1941-2014)\n",
    "                all_test_mem.append(\n",
    "                    np.corrcoef(prediction[i*74:(i+1)*74], \n",
    "                                target_test[i*74:(i+1)*74])[0][1])\n",
    "                \n",
    "            region_r_vals.append(all_test_mem)\n",
    "            \n",
    "        lag_r_vals.append(region_r_vals)\n",
    "        \n",
    "    month_r_vals.append(lag_r_vals)\n",
    "    \n",
    "month_r_vals_xr = xr.Dataset(\n",
    "    data_vars = {'r_value':(['month','lag','region','member'], month_r_vals)},\n",
    "    coords = {'month':np.arange(1,13), 'lag':np.arange(1,21), \n",
    "              'region':[1,2,3,4,5,6,11], \n",
    "              'member':SIC_CMIP6_xr['member'][-525:].values,\n",
    "             },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b60cdb8-d10c-48e0-81fb-a6e1e71900b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the r values and group by GCM\n",
    "CMIP6_GCM_list = []\n",
    "for GCM in np.sort(list(good_GCM_mem.keys())):    \n",
    "    n_mem = len(good_GCM_mem[GCM])\n",
    "    if n_mem > 2:\n",
    "        CMIP6_GCM_list.append(GCM)\n",
    "\n",
    "\n",
    "r_val_by_GCM = []\n",
    "for GCM_i, GCM in enumerate(CMIP6_GCM_list):\n",
    "    begin_mem = str(10000+(GCM_i*1000))\n",
    "    end_mem   = str(10999+(GCM_i*1000))\n",
    "    \n",
    "    data = month_r_vals_xr.sel(member=slice(begin_mem,end_mem))\n",
    "    data['member'] = np.arange(0,len(data['member']))\n",
    "    r_val_by_GCM.append(data)\n",
    "\n",
    "r_val_by_GCM = xr.concat((r_val_by_GCM), dim='model_name')\n",
    "r_val_by_GCM['model_name'] = CMIP6_GCM_list\n",
    "\n",
    "r_val_by_GCM.attrs = {\n",
    "    'Description': 'Pearson correlation coefficient for test data (3rd and '\\\n",
    "        +'later members of 42 global climate models as follows: '\\\n",
    "        +f'{CMIP6_GCM_list}. Model train and validated on the 1st and 2nd '\\\n",
    "        +'members of the same 42 GCMs. The model features were the following '\\\n",
    "        +'modes of variability: AMO, IPO, NINO34, PDO, ATN,'\\\n",
    "        +'NPO, PNA, NAO, TAS. The targets were 2 year lowpass '\\\n",
    "        +'filtered regional Arctic sea ice concentration anomalies for lag '\\\n",
    "        +'times of 1-20 years. Regions are defined by NSIDC MASIE-NH Version'\\\n",
    "        +' 1 (doi:10.7265/N5GT5K3K). The climate modes are computed by the '\\\n",
    "        +'Climate Variability Diagnostics Package (CVDP). The model was '\\\n",
    "        +'trained using an L1 loss function and Adam optimizer with 2000 '\\\n",
    "        +'epochs: nn.Sequential(nn.Linear(36,1,bias=False)), with a '\\\n",
    "        +'learning rate of 5e-4.',\n",
    "    'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': f'CMIP6 global climate models for historical simulations '\\\n",
    "        +'with sea ice concentration output, climate modes calculated by CVDP '\\\n",
    "        +'(doi:10.1002/2014EO490002)',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "        +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "\n",
    "r_val_by_GCM.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    +f'test_r_values_linear_CMIP6_var_9_all_IPO.nc'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610f1f9-bfca-4d11-b035-0b2facdb87d3",
   "metadata": {},
   "source": [
    "## Now do the same for the test members for the LEs. 'Perfect model' test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d926ddb-5c6a-4dbf-a28d-de35cf1e6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weight data \n",
    "LE_weights = {}\n",
    "for GCM in LE_GCM_list[:-1]:\n",
    "    all_months_LE = []\n",
    "    for month_ in np.arange(1,13):\n",
    "        all_1_weights_xr = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'weights_linear_{GCM}_month_{str(month_).zfill(2)}_'\\\n",
    "            +'var_9_all_IPO_lowpass_filt.nc'\n",
    "        )\n",
    "        all_months_LE.append(all_1_weights_xr['weights'])\n",
    "\n",
    "    all_months_LE = xr.concat((all_months_LE), dim='month')\n",
    "    all_months_LE['month'] = np.arange(1,13)\n",
    "    \n",
    "    LE_weights[GCM] = all_months_LE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ad1f304-3311-4154-96e5-c6f7790747c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-18 10:04:59.979893 ACCESS-ESM1-5\n",
      "2023-10-18 10:05:38.250376 CESM2-LENS\n",
      "2023-10-18 10:06:16.970812 CNRM-CM6-1\n",
      "2023-10-18 10:06:53.347515 CanESM5\n",
      "2023-10-18 10:07:30.345063 EC-Earth3\n",
      "2023-10-18 10:08:06.183012 GISS-E2-1-G\n",
      "2023-10-18 10:08:45.140309 GISS-E2-1-H\n",
      "2023-10-18 10:09:20.158155 IPSL-CM6A-LR\n",
      "2023-10-18 10:09:56.412165 MIROC-ES2L\n",
      "2023-10-18 10:10:32.358459 MIROC6\n",
      "2023-10-18 10:11:09.897982 MPI-ESM1-2-LR\n",
      "2023-10-18 10:11:46.024587 NorCPM1\n"
     ]
    }
   ],
   "source": [
    "LE_test = {}\n",
    "for GCM in LE_GCM_list[:-1]:\n",
    "    print(datetime.datetime.now(), GCM)\n",
    "    month_r_vals = []\n",
    "    for month_ in np.arange(1,13):\n",
    "        \n",
    "        lag_r_vals = []\n",
    "        for lag_ in np.arange(1,21):\n",
    "\n",
    "            CVDP_train, CVDP_valid, CVDP_test = load_CVDP(\n",
    "                GCM, month_, lag_, [1941,2014], \n",
    "                extra_drop=None, white_noise=False,\n",
    "            )\n",
    "\n",
    "            # CVDP_test = torch.cat(\n",
    "            #     (CVDP_test[:,0:5], CVDP_test[:,8:29], CVDP_test[:,32:]), dim=1\n",
    "            # )\n",
    "            CVDP_test = torch.cat(\n",
    "                (CVDP_test[:,0:16], CVDP_test[:,20:24], \n",
    "                 CVDP_test[:,36:48], CVDP_test[:,52:]), dim=1\n",
    "            )\n",
    "\n",
    "            region_r_vals = []\n",
    "            for region_ in [1,2,3,4,5,6,11]:\n",
    "                target_train, target_valid, target_test = load_SIC(\n",
    "                     GCM, month_, region_, [1941,2014], \n",
    "                )\n",
    "\n",
    "                prediction = np.sum(\n",
    "                    np.array(CVDP_test) \n",
    "                    * np.tile(LE_weights[GCM].sel(month=month_).sel(\n",
    "                        region=region_).sel(lag=lag_), \n",
    "                    [CVDP_test.shape[0],1]), axis=1\n",
    "                )\n",
    "\n",
    "                all_test_mem = []\n",
    "                for i in range(int(CVDP_test.shape[0]/74)):\n",
    "                    all_test_mem.append(\n",
    "                        np.corrcoef(prediction[i*74:(i+1)*74], \n",
    "                                    target_test[i*74:(i+1)*74])[0][1])\n",
    "\n",
    "                region_r_vals.append(all_test_mem)\n",
    "\n",
    "            lag_r_vals.append(region_r_vals)\n",
    "\n",
    "        month_r_vals.append(lag_r_vals)\n",
    "\n",
    "    month_r_vals_xr = xr.DataArray(\n",
    "        data = month_r_vals,\n",
    "        dims = ['month', 'lag', 'region', 'member'],\n",
    "        coords = {\n",
    "            'month':np.arange(1,13), \n",
    "            'lag':np.arange(1,21), \n",
    "            'region':[1,2,3,4,5,6,11],\n",
    "            'member':np.arange(CVDP_test.shape[0]/74),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    LE_test[GCM] = month_r_vals_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be60cd91-e7e4-4de7-8170-dcb4e89c053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LE_test = xr.Dataset(LE_test)\n",
    "\n",
    "LE_test.attrs = {\n",
    "    'Description': 'Pearson correlation coefficient for test data (final 10% '\\\n",
    "        +'of members from 12 GCM large ensembles as per the data variables. '\\\n",
    "        +'The linear model was train and validated on the first 75% and 15% '\\\n",
    "        +'of members of the GCM. The model features were the following '\\\n",
    "        +'modes of variability: AMO, IPO, NINO34, PDO, ATN, '\\\n",
    "        +'NPO, PNA, NAO, TAS. The targets were 2 year lowpass '\\\n",
    "        +'filtered regional Arctic sea ice concentration anomalies for lag '\\\n",
    "        +'times of 1-20 years. Regions are defined by NSIDC MASIE-NH Version'\\\n",
    "        +' 1 (doi:10.7265/N5GT5K3K). The climate modes are computed by the '\\\n",
    "        +'Climate Variability Diagnostics Package (CVDP). The model was '\\\n",
    "        +'trained using an L1 loss function and Adam optimizer with 2000 '\\\n",
    "        +'epochs: nn.Sequential(nn.Linear(36,1,bias=False)), with a '\\\n",
    "        +'learning rate of 5e-4.',\n",
    "    'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': f'CMIP6 global climate models for historical simulations '\\\n",
    "        +'with sea ice concentration output, climate modes calculated by CVDP '\\\n",
    "        +'(doi:10.1002/2014EO490002)',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "        +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "\n",
    "LE_test.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    +f'test_r_values_linear_LEs_var_9_all_IPO.nc'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d95d9-0051-4fa9-b30e-b7d3920c5e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Remove one variable at a time from the LE linear models, retrain on the useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "5a536d7a-8e1f-41bd-ac43-4d2ce5d158af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model_remove(model_name, month_, region_list, lag_list, \n",
    "                              start_end_yr, n_epoch, learn_rate, \n",
    "                              extra_drop_=None, white_noise_=None):\n",
    "    '''\n",
    "    Train the 4 machine learning models for a given large ensemble on a \n",
    "    specificed of sea ice concentration data for a specified set of months,\n",
    "    regions, and lags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The months of sea ice concentration anomalies on which to train the \n",
    "        machine learning model. \n",
    "    region_list: list of ints,\n",
    "        A list of the regional sea ice anomalies to train the model on \n",
    "        separately.\n",
    "    lag_list: list of ints,\n",
    "        The range of lagged year on which to separately train the machine \n",
    "        learning model \n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (inclusive) for the sea ice concentration data.\n",
    "    extra_drop: none or list of strings,\n",
    "        If False, no additonal variables are dropped. If a list of a string or\n",
    "        strings, those variables listed will be removed from the CVDP data. \n",
    "    white_noise: bool,\n",
    "        If True, add a variable of normalized random values in a gaussian \n",
    "        distribution for the 4 seasons, all members and all years.\n",
    "    n_epoch: int, \n",
    "        number of epochs with which to train all of the machine learning \n",
    "        models.\n",
    "    learn_rate: int,\n",
    "        Integer corresponding to the learning rate.         \n",
    "                      \n",
    "    Returns\n",
    "    ----------\n",
    "    r_values_xr: xarray.Dataset,\n",
    "        The pearson correlation coefficients for the linear model\n",
    "        for the validation data.\n",
    "    all_1_grads_xr: xarray.Dataset,\n",
    "        The gradients from the machine learning model using a simple multiple\n",
    "        linear regression model.    \n",
    "    '''\n",
    "    if white_noise_:\n",
    "        fewer_var = 0\n",
    "    else:\n",
    "        fewer_var = 1\n",
    "    \n",
    "    if type(extra_drop_) != type(None):\n",
    "        n_features = int((51-len(extra_drop_)-fewer_var))\n",
    "    else:\n",
    "        n_features = int(51-fewer_var)\n",
    "    \n",
    "    if model_name in ['CMIP6','CMIP6_30']:\n",
    "        doi_model = '10.5194/gmd-9-1937-2016'\n",
    "    else:\n",
    "        doi_model = CMIP6_info['doi'].sel(model=model_name).values\n",
    "    \n",
    "    all_1_weights = np.empty([len(region_list), len(lag_list), n_features], \n",
    "                             dtype=float)\n",
    "    \n",
    "    all_r_values = np.empty([len(region_list), len(lag_list)], dtype=float)\n",
    "\n",
    "    for region_i, region_ in enumerate(region_list):\n",
    "        \n",
    "        for lag_i, lag_ in enumerate(lag_list):\n",
    "            \n",
    "            #load the feature and target data for the correct model, month,\n",
    "            #region and lag\n",
    "            CVDP_train, CVDP_valid, CVDP_test = load_CVDP(\n",
    "                model_name, month_, lag_, start_end_yr, \n",
    "                extra_drop=extra_drop_, white_noise=white_noise_\n",
    "            )\n",
    "            \n",
    "            CVDP_train = torch.cat(\n",
    "                (CVDP_train[:,0:5], CVDP_train[:,8:29], CVDP_train[:,32:]),\n",
    "                dim=1)\n",
    "            CVDP_valid = torch.cat(\n",
    "                (CVDP_valid[:,0:5], CVDP_valid[:,8:29], CVDP_valid[:,32:]),\n",
    "                dim=1)\n",
    "            \n",
    "            if extra_drop_: #remove seasonal var exactly matching extra_drop\n",
    "                remove_i = np.where(\n",
    "                    np.char.find(var_list_use, extra_drop_)==0)[0][0]\n",
    "            \n",
    "                CVDP_train = torch.cat(\n",
    "                    (CVDP_train[:,0:remove_i], CVDP_train[:,remove_i+1:]), dim=1)\n",
    "                CVDP_valid = torch.cat(\n",
    "                    (CVDP_valid[:,0:remove_i], CVDP_valid[:,remove_i+1:]), dim=1)\n",
    "                        \n",
    "            if white_noise_:\n",
    "                CVDP_train = CVDP_train[:,:-3]\n",
    "                CVDP_valid = CVDP_valid[:,:-3]\n",
    "                        \n",
    "            \n",
    "            target_train, target_valid, target_test = load_SIC(\n",
    "                model_name, month_, region_, start_end_yr, \n",
    "            )\n",
    "\n",
    "            ML_model_computed = []\n",
    "            ML_model_use = nn.Sequential(nn.Linear(n_features,1,bias=False))\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                params=ML_model_use.parameters(), \n",
    "                lr=learn_rate\n",
    "            )\n",
    "\n",
    "            ##### train the model #####\n",
    "            train_r, valid_r = [],[]\n",
    "            for epoch in range(n_epoch):\n",
    "                # TRAIN\n",
    "                prediction = ML_model_use(CVDP_train)\n",
    "                optimizer.zero_grad() #reset the gradients to zeros\n",
    "                loss = loss_fcn(prediction[:,0].double(), target_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_r.append(\n",
    "                    np.corrcoef(prediction[:,0].detach().numpy(),\n",
    "                                target_train.detach().numpy()\n",
    "                               )[1][0]\n",
    "                )\n",
    "\n",
    "                ML_model_computed.append(ML_model_use)\n",
    "\n",
    "                ##### validate the model #####\n",
    "                with torch.no_grad():\n",
    "                    p_val = ML_model_use(CVDP_valid)\n",
    "                    loss_val = loss_fcn(p_val[:,0].double(), target_valid)\n",
    "                    valid_r.append(np.corrcoef(p_val[:,0].detach().numpy(),\n",
    "                                    target_valid.detach().numpy())[1][0])\n",
    "\n",
    "                    #select highest validation r value from all epochs\n",
    "                    all_r_values[region_i][lag_i] = np.max(valid_r)\n",
    "\n",
    "                    all_1_weights[region_i][lag_i] = np.ravel(\n",
    "                        ML_model_computed[np.argmax(valid_r)][                               \n",
    "                            0].weight[0,:].detach().numpy())\n",
    "\n",
    "    if model_name in ['CMIP6','CMIP6_30']:\n",
    "        model_name = 'CMIP6 multi-model large ensemble'\n",
    "        \n",
    "    if (white_noise_ == None) and (extra_drop_ == None):\n",
    "        # mode_month_list = [\n",
    "        #     item for item in var_month_list if 'RAND' not in item]\n",
    "        mode_month_list = var_list_use[:-1]\n",
    "    elif (len(extra_drop_) == 1) and (white_noise_ == True):\n",
    "        # mode_month_list = [\n",
    "        #     item for item in var_month_list if extra_drop_[0] not in item]\n",
    "        mode_month_list = np.delete(np.array(var_list_use).copy(),remove_i)\n",
    "    else:\n",
    "        extra_drop_inc_rand = np.append(extra_drop_, 'RAND')\n",
    "        mode_month_list = var_month_list.copy()\n",
    "        for i in extra_drop_inc_rand:\n",
    "            mode_month_list = [\n",
    "                item for item in mode_month_list if i not in item]\n",
    "\n",
    "    \n",
    "    #save the r values to NetCDF every model and month\n",
    "    r_values_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'r_value':(['region', 'lag'], all_r_values)},\n",
    "        coords = {\n",
    "            'region':region_list, 'lag':lag_list},\n",
    "    )\n",
    "\n",
    "    #save the linear 1 layer neural network weights to NetCDF\n",
    "    all_1_weights_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'weights':(['region', 'lag', 'mode_month'], all_1_weights),\n",
    "        },\n",
    "        coords = {'region':region_list, 'lag':lag_list, \n",
    "                  'mode_month':mode_month_list,\n",
    "        },\n",
    "    )\n",
    "        \n",
    "    return(r_values_xr, all_1_weights_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab3ea2-abeb-40da-a746-bb3902044a88",
   "metadata": {},
   "source": [
    "## Remove one variable at a time from the LE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a017572a-8b14-456b-9353-930d00bfa1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the following dictionaries to determine which months and regions to \n",
    "#retrain the model on. Based on best 5 year lag r2 above persistence. \n",
    "\n",
    "#use the months from the best CMIP6 values\n",
    "good_GCM_months = {\n",
    "    '1': [ 9,  9,  9,  9,  9,  9,],\n",
    "    '2': [10, 10, 10, 10, 10, 10,],\n",
    "    '3': [ 9,  9,  9,  9,  9,],\n",
    "    '4': [10, 10,],\n",
    "    '5': [10, 10, 10, 10, 10,],\n",
    "    '6': [ 1,  1,],\n",
    "    '11':[ 8,  8,  8, 8,],\n",
    "}\n",
    "\n",
    "good_GCM_names = {\n",
    "    '1': ['CanESM5', 'CESM2-LENS', 'MIROC6', 'GISS-E2-1-G', 'IPSL-CM6A-LR', \n",
    "          'GISS-E2-1-H'],\n",
    "    '2': ['CanESM5', 'CESM2-LENS', 'MIROC6', 'GISS-E2-1-G', 'IPSL-CM6A-LR',\n",
    "          'MIROC-ES2L'],\n",
    "    '3': ['CanESM5', 'MIROC6', 'ACCESS-ESM1-5', 'IPSL-CM6A-LR', 'MIROC-ES2L'],\n",
    "    '4': ['CanESM5', 'IPSL-CM6A-LR'],\n",
    "    '5': ['CanESM5', 'CESM2-LENS', 'MIROC6', 'MPI-ESM1-2-LR', 'GISS-E2-1-H'],\n",
    "    '6': ['CanESM5', 'GISS-E2-1-H'],\n",
    "   '11': ['CanESM5', 'MIROC6', 'GISS-E2-1-G', 'ACCESS-ESM1-5']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ae8440b8-ec6e-4e03-bd6d-938e2735bb73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-09 13:03:10.320037 1\n",
      "2023-03-09 13:03:10.320097 CanESM5\n",
      "2023-03-09 13:03:10.321794 CESM2-LENS\n",
      "2023-03-09 13:03:10.322632 MIROC6\n",
      "2023-03-09 13:03:10.323275 GISS-E2-1-G\n",
      "2023-03-09 13:03:10.323908 IPSL-CM6A-LR\n",
      "2023-03-09 13:03:10.324530 GISS-E2-1-H\n",
      "2023-03-09 13:03:10.325166 2\n",
      "2023-03-09 13:03:10.325207 CanESM5\n",
      "2023-03-09 13:03:10.325816 CESM2-LENS\n",
      "2023-03-09 13:03:10.326445 MIROC6\n",
      "2023-03-09 13:03:10.327074 GISS-E2-1-G\n",
      "2023-03-09 13:03:10.327697 IPSL-CM6A-LR\n",
      "2023-03-09 13:03:10.328353 MIROC-ES2L\n",
      "2023-03-09 13:03:10.329039 3\n",
      "2023-03-09 13:03:10.329082 CanESM5\n",
      "2023-03-09 13:03:10.329746 MIROC6\n",
      "2023-03-09 13:03:10.330453 ACCESS-ESM1-5\n",
      "2023-03-09 13:03:10.331127 IPSL-CM6A-LR\n",
      "2023-03-09 13:03:10.331775 MIROC-ES2L\n",
      "2023-03-09 13:03:10.337396 4\n",
      "2023-03-09 13:03:10.337441 CanESM5\n",
      "2023-03-09 13:03:10.338098 IPSL-CM6A-LR\n",
      "2023-03-09 13:03:10.338724 5\n",
      "2023-03-09 13:03:10.338765 CanESM5\n",
      "2023-03-09 13:03:10.339385 CESM2-LENS\n",
      "2023-03-09 13:03:10.340020 MIROC6\n",
      "2023-03-09 13:03:10.340645 MPI-ESM1-2-LR\n",
      "2023-03-09 13:03:10.341276 GISS-E2-1-H\n",
      "2023-03-09 13:03:10.341903 6\n",
      "2023-03-09 13:03:10.341943 CanESM5\n",
      "2023-03-09 13:03:10.342546 GISS-E2-1-H\n",
      "2023-03-09 13:03:10.343178 11\n",
      "2023-03-09 13:03:10.343220 CanESM5\n",
      "2023-03-09 13:03:10.343824 MIROC6\n",
      "2023-03-09 13:03:10.344519 GISS-E2-1-G\n",
      "2023-03-09 13:03:10.345188 ACCESS-ESM1-5\n"
     ]
    }
   ],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "\n",
    "var_list_use = np.delete(np.array(var_month_list[:-3]).copy(),[5,6,7,29,30,31])\n",
    "\n",
    "for region_key in [1,2,3,4,5,6,11]:\n",
    "    print(datetime.datetime.now(), region_key)\n",
    "    \n",
    "    for GCM_i, GCM in enumerate(good_GCM_names[str(region_key)]):\n",
    "        print(datetime.datetime.now(), GCM)\n",
    "        \n",
    "        doi_model = CMIP6_info['doi'].sel(model=GCM).values\n",
    "\n",
    "        month__ = good_GCM_months[str(region_key)][GCM_i]\n",
    "        \n",
    "        #skip this one if already run this analysis\n",
    "        if len(glob.glob(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'validation_r_values_linear_{GCM}_month_{str(month__).zfill(2)}_'\\\n",
    "            +f'var_15_drop_1_season_region_{region_key}.nc')):\n",
    "            continue\n",
    "\n",
    "        r_values_list = []\n",
    "        weights_list  = []\n",
    "        for drop_var in var_list_use:\n",
    "            print(datetime.datetime.now(), drop_var)\n",
    "\n",
    "            if drop_var == 'RAND_1':\n",
    "                r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                    model_name=GCM, month_=month__, region_list=[region_key], \n",
    "                    lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                    n_epoch=2000, learn_rate=5e-4, extra_drop_=None,\n",
    "                    white_noise_=None,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                    model_name=GCM, month_=month__, region_list=[region_key], \n",
    "                    lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                    n_epoch=2000, learn_rate=5e-4, extra_drop_=[drop_var],\n",
    "                    white_noise_=True,\n",
    "                )  \n",
    "\n",
    "            r_values_list.append(r_values_xr)\n",
    "            weights_list.append(all_weights_xr)\n",
    "\n",
    "        r_values_save = xr.concat((r_values_list), dim='drop_var')\n",
    "        r_values_save['drop_var'] = var_list_use\n",
    "\n",
    "        weights_save = xr.concat((weights_list), dim='drop_var')\n",
    "        weights_save['drop_var'] = var_list_use\n",
    "\n",
    "        r_values_attrs = {\n",
    "            'Description': 'Pearson correlation coefficient for validation '\\\n",
    "                +f'data (15%) of members from the global climate model {GCM}. '\\\n",
    "                +'14 of the 15 seasonal (except NPI and IPO) modes '\\\n",
    "                +'of variability as computed by the Climate Variability '\\\n",
    "                +'Diagnostics Package (CVDP) with 2 year lowpass filtered '\\\n",
    "                +'sea ice concentration. These climate modes are as follows: '\\\n",
    "                +'AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, NPO, PNA, '\\\n",
    "                +'NAO, SAM, TAS, RAND. The linear model is trained at '\\\n",
    "                +'different lag times of 1-20 years and for each '\\\n",
    "                +'region (regions are defined by NSIDC MASIE-NH Version 1 '\\\n",
    "                +'(doi:10.7265/N5GT5K3K). Using an L1 loss function and Adam '\\\n",
    "                +'optimizer with 2000 epochs: nn.Sequential(nn.Linear(50,1,'\\\n",
    "                +'bias=False)), learning rate = 5e-4.',\n",
    "            'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "                \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "            'Data source': f'CMIP6 global climate model {GCM}, doi:'\\\n",
    "                +f'{doi_model} for historical sea ice concentration '\\\n",
    "                +'simulation, climate modes calculated by CVDP '\\\n",
    "                +'(doi:10.1002/2014EO490002)',\n",
    "            'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "                +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "        }\n",
    "\n",
    "        r_values_save.attrs = r_values_attrs\n",
    "        r_values_save.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'validation_r_values_linear_{GCM}_month_{str(month__).zfill(2)}_'\\\n",
    "            +f'var_15_drop_1_season_region_{region_key}.nc'\n",
    "        )\n",
    "\n",
    "\n",
    "        weights_attrs = r_values_attrs.copy()\n",
    "        weights_attrs['Description'] = 'Linear weights including 14 of the '\\\n",
    "            +'15 seasonal (except NPI and IPO) modes of variability as '\\\n",
    "            +'computed by the Climate Variability '\\\n",
    "            +'Diagnostics Package (CVDP) with 2 year lowpass filtered sea ice '\\\n",
    "            +'concentration. The linear model is trained/validated on the '\\\n",
    "            +'first 75/15% of the members from the global climate model '\\\n",
    "            +f'{GCM}. The climate modes are as follows: AMO, IPO, NINO34, '\\\n",
    "            +'PDO, AMM, ATN, IOD, NPI, NAM, NPO, PNA, NAO, SAM, TAS, RAND. '\\\n",
    "            +'The linear model is trained at different lag times of 1-20 '\\\n",
    "            +'years and for each region (regions are defined by NSIDC '\\\n",
    "            +'MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). Using an L1 loss '\\\n",
    "            +'function and Adam optimizer with 2000 epochs: '\\\n",
    "            +'nn.Sequential(nn.Linear(50,1,bias=False)), learning rate = 5e-4.'\n",
    "\n",
    "        weights_save.attrs = weights_attrs\n",
    "        weights_save.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'weights_linear_{GCM}_month_{str(month__).zfill(2)}_'\\\n",
    "            +f'var_15_drop_1_season_region_{region_key}.nc'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e2d5d-2c33-49bc-8d1c-94b23c25e644",
   "metadata": {},
   "source": [
    "### Remove all variables with a worse effect on skill than a random variable for the LEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05a6030-e036-4db6-a6a3-3e6f11e68e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "          +'best_season_MMLE_3_lags_2_20_using_drop_1.pickle', 'rb') as handle:\n",
    "    best_seasons_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4352525d-328e-46a6-86da-77e831f007ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_useful_season(\n",
    "    model_name, month_, region_, best_seasons, lag_list, start_end_yr,\n",
    "    n_epoch, learn_rate):\n",
    "    \n",
    "    var_list_to_use = list(best_seasons_dict[str(region_)].keys())\n",
    "    \n",
    "    n_features = 14\n",
    "    \n",
    "    if model_name in ['CMIP6','CMIP6_30']:\n",
    "        doi_model = '10.5194/gmd-9-1937-2016'\n",
    "    else:\n",
    "        doi_model = CMIP6_info['doi'].sel(model=model_name).values\n",
    "    \n",
    "    all_1_weights = np.empty([len(lag_list), n_features], \n",
    "                             dtype=float)\n",
    "    \n",
    "    all_r_values = np.empty([len(lag_list)], dtype=float)\n",
    "        \n",
    "    for lag_i, lag_ in enumerate(lag_list):\n",
    "\n",
    "        #load the feature and target data for the correct model, month,\n",
    "        #region and lag\n",
    "        CVDP_year_month = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "            +'CVDP_standardized_linear_detrended_1920_2014_historical_'\\\n",
    "            +f'{model_name}.nc'\n",
    "        )    \n",
    "\n",
    "        #select only the CVDP varaibles that are useful\n",
    "        CVDP_year_month = CVDP_year_month.to_array('variable').sortby(\n",
    "            'time')\n",
    "\n",
    "        month_seperate = []\n",
    "        for i in [1,4,7,10]:\n",
    "            temp_data = CVDP_year_month.sel(\n",
    "                time=CVDP_year_month['time.month']==i)\n",
    "            temp_data['time'] = np.arange(1920,2015)\n",
    "            month_seperate.append(temp_data)\n",
    "\n",
    "        CVDP_data = xr.concat((month_seperate), dim='month')\n",
    "        CVDP_data['month'] = [1,4,7,10]\n",
    "        CVDP_data = CVDP_data.rename({'time':'year'})\n",
    "\n",
    "        #now stack the CVDP data into X members and years, Y features\n",
    "        CVDP_train = []\n",
    "        CVDP_valid = []\n",
    "        for var_ in var_list_to_use:\n",
    "            lag_season = best_seasons_dict[str(region_)][var_]\n",
    "\n",
    "            if lag_season >= month_:\n",
    "                extra_year = 1\n",
    "            else:\n",
    "                extra_year = 0\n",
    "\n",
    "            CVDP_month_data = CVDP_data.sortby('member')\n",
    "\n",
    "            CVDP_month_data = CVDP_month_data.sel(\n",
    "                month=lag_season).sel(variable=var_).sel(\n",
    "                year=slice(str(start_end_yr[0]-lag_-extra_year), \n",
    "                           str(start_end_yr[1]-lag_-extra_year)))\n",
    "            CVDP_month_data['year'] = np.arange(\n",
    "                0,start_end_yr[1]-start_end_yr[0]+1)\n",
    "            \n",
    "            CVDP_train.append(CVDP_month_data.sel(\n",
    "                member=LE_train_mem[model_name]))\n",
    "            CVDP_valid.append(CVDP_month_data.sel(\n",
    "                member=LE_valid_mem[model_name]))\n",
    "\n",
    "        CVDP_train_stacked = xr.concat((CVDP_train),'variable').stack(\n",
    "            member_time=('member','year'))   \n",
    "        CVDP_train = torch.Tensor(CVDP_train_stacked.transpose().values)\n",
    "        \n",
    "        CVDP_valid_stacked = xr.concat((CVDP_valid),'variable').stack(\n",
    "            member_time=('member','year'))\n",
    "        CVDP_valid = torch.Tensor(CVDP_valid_stacked.transpose().values)\n",
    "\n",
    "\n",
    "        #load the SIC data\n",
    "        target_train, target_valid, target_test = load_SIC(\n",
    "            model_name, month_, region_, start_end_yr, \n",
    "        )\n",
    "        \n",
    "        #prepare to train the linear model\n",
    "        ML_model_computed = []\n",
    "        ML_model_use = nn.Sequential(nn.Linear(n_features,1,bias=False))\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=ML_model_use.parameters(), lr=learn_rate\n",
    "        )\n",
    "\n",
    "        ##### train the model #####\n",
    "        valid_r, valid_loss = [],[]\n",
    "        for epoch in range(n_epoch):\n",
    "            # TRAIN\n",
    "            prediction = ML_model_use(CVDP_train)\n",
    "            optimizer.zero_grad() #reset the gradients to zeros\n",
    "            loss = loss_fcn(prediction[:,0].double(), target_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ML_model_computed.append(ML_model_use)\n",
    "\n",
    "            ##### validate the model #####\n",
    "            with torch.no_grad():\n",
    "                p_val = ML_model_use(CVDP_valid)\n",
    "                loss_val = loss_fcn(p_val[:,0].double(), target_valid)\n",
    "                \n",
    "                valid_loss.append(loss_val)\n",
    "                valid_r.append(np.corrcoef(p_val[:,0].detach().numpy(),\n",
    "                                target_valid.detach().numpy())[1][0])\n",
    "\n",
    "                #select the lowest validation loss function from all epochs\n",
    "                all_r_values[lag_i] = valid_r[np.argmin(valid_loss)]\n",
    "\n",
    "                all_1_weights[lag_i] = np.ravel(\n",
    "                    ML_model_computed[np.argmin(valid_loss)][0].weight[\n",
    "                        0,:].detach().numpy())\n",
    "\n",
    "    #save the r values\n",
    "    r_values_xr = xr.Dataset(\n",
    "        data_vars = {'r_value':(['lag'], all_r_values)},\n",
    "        coords = {'lag':lag_list},\n",
    "    )\n",
    "\n",
    "    #save the linear model weights to NetCDF\n",
    "    all_1_weights_xr = xr.Dataset(\n",
    "        data_vars = {'weights':(['lag', 'mode_month'], all_1_weights), },\n",
    "        coords = {'lag':lag_list, 'mode_month':var_list_to_use,},\n",
    "    )\n",
    "        \n",
    "    return(r_values_xr, all_1_weights_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dbc5b83-5dd7-4ca7-9cb5-74c92ae7eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-12 08:34:44.098146 2\n",
      "2023-03-12 08:34:44.098260 CanESM5\n",
      "2023-03-12 08:40:07.686749 CESM2-LENS\n",
      "2023-03-12 08:45:32.221758 MIROC6\n",
      "2023-03-12 08:51:00.016699 GISS-E2-1-G\n",
      "2023-03-12 08:56:26.582114 IPSL-CM6A-LR\n",
      "2023-03-12 09:01:49.879249 MIROC-ES2L\n",
      "2023-03-12 09:07:13.078281 3\n",
      "2023-03-12 09:07:13.078495 CanESM5\n",
      "2023-03-12 09:12:37.781233 MIROC6\n",
      "2023-03-12 09:18:02.065041 ACCESS-ESM1-5\n",
      "2023-03-12 09:23:25.024493 IPSL-CM6A-LR\n",
      "2023-03-12 09:28:48.349601 MIROC-ES2L\n",
      "2023-03-12 09:34:09.434481 4\n",
      "2023-03-12 09:34:09.434700 CanESM5\n",
      "2023-03-12 09:39:32.117619 IPSL-CM6A-LR\n",
      "2023-03-12 09:44:52.727347 5\n",
      "2023-03-12 09:44:52.727524 CanESM5\n",
      "2023-03-12 09:50:16.965656 CESM2-LENS\n",
      "2023-03-12 09:55:38.767928 MIROC6\n",
      "2023-03-12 10:01:01.103115 MPI-ESM1-2-LR\n",
      "2023-03-12 10:06:21.618735 GISS-E2-1-H\n",
      "2023-03-12 10:11:42.766342 6\n",
      "2023-03-12 10:11:42.766537 CanESM5\n",
      "2023-03-12 10:17:06.473921 GISS-E2-1-H\n",
      "2023-03-12 10:22:28.301052 11\n",
      "2023-03-12 10:22:28.301229 CanESM5\n",
      "2023-03-12 10:27:50.396612 MIROC6\n",
      "2023-03-12 10:33:14.373888 GISS-E2-1-G\n",
      "2023-03-12 10:38:37.773094 ACCESS-ESM1-5\n"
     ]
    }
   ],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "\n",
    "month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "               'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "for region_key in [2,3,4,5,6,11]:\n",
    "    print(datetime.datetime.now(), region_key)\n",
    "    \n",
    "    month__ = good_GCM_months[str(region_key)][0]\n",
    "    \n",
    "    GCM_r_vals = []\n",
    "    GCM_weights = []\n",
    "    for GCM_i, GCM in enumerate(good_GCM_names[str(region_key)]):\n",
    "        print(datetime.datetime.now(), GCM)\n",
    "        \n",
    "        doi_model = CMIP6_info['doi'].sel(model=GCM).values\n",
    "    \n",
    "        r_values_xr, all_1_weights_xr = train_linear_useful_season(\n",
    "            model_name=GCM, month_=month__, region_=region_key, \n",
    "            best_seasons=best_seasons_dict, lag_list=np.arange(1,21), \n",
    "            start_end_yr=[1941,2014], n_epoch=2000, learn_rate=2e-4,\n",
    "        )\n",
    "        \n",
    "        GCM_r_vals.append(r_values_xr)\n",
    "        GCM_weights.append(all_1_weights_xr)\n",
    "        \n",
    "    region_r_vals = xr.concat((GCM_r_vals), dim='model_name')\n",
    "    region_r_vals['model_name'] = good_GCM_names[str(region_key)]\n",
    "    \n",
    "    region_weights = xr.concat((GCM_weights), dim='model_name')\n",
    "    region_weights['model_name'] = good_GCM_names[str(region_key)]\n",
    "\n",
    "    r_values_attrs = {\n",
    "        'Description': 'Pearson correlation coefficient for validation '\\\n",
    "            +'data (15%) of members from the global climate models as '\\\n",
    "            +f'follows {good_GCM_names[str(region_key)]} for '\\\n",
    "            +f'{month_names[month__-1]}, with the highest predictability '\\\n",
    "            +'season of each mode of variability chosen: '\\\n",
    "            +f'{best_seasons_dict[str(region_key)]}. Modes computed by the '\\\n",
    "            +'Climate Variability Diagnostics Package (CVDP) with 2 year '\\\n",
    "            +'lowpass filtered sea ice concentration. The linear model is '\\\n",
    "            +'trained at  different lag times of 1-20 years, the region is '\\\n",
    "            +f'{region_key} as defined by NSIDC MASIE-NH Version 1 '\\\n",
    "            +'(doi:10.7265/N5GT5K3K). Training uses an L1 loss function and '\\\n",
    "            +'epochs: nn.Sequential(nn.Linear(14,1,bias=False)), '\\\n",
    "            +'Adam optimizer with 2000 learning rate = 2e-4.',\n",
    "        'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "            \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "        'Data source': f'CMIP6 global climate model historical simulations '\\\n",
    "            +'for historical sea ice concentration, with climate modes '\\\n",
    "            +'calculated by CVDP (doi:10.1002/2014EO490002)',\n",
    "        'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "            +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "    }\n",
    "\n",
    "    region_r_vals.attrs = r_values_attrs\n",
    "    region_r_vals.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_linear_LEs_region_{region_key}_month_{month__}'\\\n",
    "        +f'best_season_only.nc'\n",
    "    )\n",
    "\n",
    "\n",
    "    weights_attrs = r_values_attrs.copy()\n",
    "    weights_attrs['Description'] = 'Linear weights of the useful seasons of '\\\n",
    "        +'modes of variability as computed by the Climate Variability '\\\n",
    "        +'Diagnoistics Package (CVDP) as follows: '\\\n",
    "        +f'{best_seasons_dict[str(region_key)]}. The target is 2 year lowpass '\\\n",
    "        +f'filtered sea ice concentrations for {month_names[month__-1]}. '\\\n",
    "        +'The linear model is trained/validated on the first 75/15% of the '\\\n",
    "        +'members from the global climate models '\\\n",
    "        +f'{good_GCM_names[str(region_key)]}. The linear model is trained at '\\\n",
    "        +f'different lag times of 1-20 years and for region {region_key} as'\\\n",
    "        +'defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "        +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "        +'epochs: nn.Sequential(nn.Linear(14,1,bias=False)), '\\\n",
    "        +'learning rate = 5e-4.'\n",
    "\n",
    "    region_weights.attrs = weights_attrs\n",
    "    region_weights.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_LEs_region_{region_key}_month_{month__}_'\\\n",
    "        +f'best_season_only.nc'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf65945-a3e8-4fb1-a68c-9dd504e1a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "# lag_range = np.arange(5,11)\n",
    "\n",
    "# month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "#                'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# for region_key in [1,2,3,4,5,6,11]:\n",
    "#     print(datetime.datetime.now(), region_key)\n",
    "    \n",
    "#     GCM_r_vals = []\n",
    "#     GCM_weights = []\n",
    "#     drop_dict = {}\n",
    "#     for GCM_i, GCM in enumerate(good_GCM_names[str(region_key)]):\n",
    "        \n",
    "#         doi_model = CMIP6_info['doi'].sel(model=GCM).values\n",
    "\n",
    "#         month__ = good_GCM_months[str(region_key)][GCM_i]\n",
    "        \n",
    "#         r_diff = r_vals_LE_drop1.sel(region=region_key).sel(lag=lag_range).mean(\n",
    "#             'lag').sel(model_name=GCM)**2 - r_vals_LE_drop1.sel(\n",
    "#             region=region_key).sel(lag=lag_range).mean('lag').sel(\n",
    "#             model_name=GCM).sel(drop_var='RAND')**2\n",
    "\n",
    "#         drop_list = r_diff['drop_var'].where(r_diff>0, drop=True).values\n",
    "#         drop_dict[GCM] = drop_list\n",
    "#         print(datetime.datetime.now(), GCM, drop_list)\n",
    "        \n",
    "#         r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "#             model_name=GCM, month_=month__, region_list=[region_key], \n",
    "#             lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "#             n_epoch=2000, learn_rate=5e-4, extra_drop_=drop_list,\n",
    "#             white_noise_=False,\n",
    "#         )  \n",
    "        \n",
    "#         GCM_r_vals.append(r_values_xr)\n",
    "#         GCM_weights.append(all_weights_xr)\n",
    "        \n",
    "#     region_r_vals = xr.concat((GCM_r_vals), dim='model_name')\n",
    "#     region_r_vals['model_name'] = good_GCM_names[str(region_key)]\n",
    "    \n",
    "#     region_weights = xr.concat((GCM_weights), dim='model_name')\n",
    "#     region_weights['model_name'] = good_GCM_names[str(region_key)]\n",
    "\n",
    "#     r_values_attrs = {\n",
    "#         'Description': 'Pearson correlation coefficient for validation '\\\n",
    "#             +'data (15%) of members from the global climate models as '\\\n",
    "#             +f'follows {good_GCM_names[str(region_key)]} for '\\\n",
    "#             +f'{month_names[month__-1]}, with modes of '\\\n",
    "#             +f'variability yielding lower predictive skill than a random '\\\n",
    "#             +f'variable dropped as follows: {drop_dict}. The full list of 15 '\\\n",
    "#             +'variables is: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "#             +'NPO, PNA, NAO, SAM, TAS, as computed by the Climate Variability '\\\n",
    "#             +'Diagnostics Package (CVDP) with 2 year lowpass filtered '\\\n",
    "#             +'sea ice concentration. The linear model is trained at '\\\n",
    "#             +f'different lag times of 1-20 years, the region is {region_key} '\\\n",
    "#             +'as defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "#             +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "#             +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "#             +'learning rate = 5e-4.',\n",
    "#         'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "#             \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "#         'Data source': f'CMIP6 global climate model historical simulations '\\\n",
    "#             +'for historical sea ice concentration, with climate modes '\\\n",
    "#             +'calculated by CVDP (doi:10.1002/2014EO490002)',\n",
    "#         'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "#             +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "#     }\n",
    "\n",
    "#     region_r_vals.attrs = r_values_attrs\n",
    "#     region_r_vals.to_netcdf(\n",
    "#         '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "#         +f'validation_r_values_linear_LEs_region_{region_key}_month_{month__}'\\\n",
    "#         +f'dropped_useless_vars.nc'\n",
    "#     )\n",
    "\n",
    "\n",
    "#     weights_attrs = r_values_attrs.copy()\n",
    "#     weights_attrs['Description'] = 'Linear weights of the useful modes of '\\\n",
    "#         +'variability as computed by the Climate Variability Diagnoistics'\\\n",
    "#         +'Package (CVDP) with 2 year lowpass filtered sea ice concentration '\\\n",
    "#         +f'for {month_names[month__-1]}. The linear '\\\n",
    "#         +f'model is trained/validated on the first 75/15% of the members '\\\n",
    "#         +f'from the global climate models {good_GCM_names[str(region_key)]}. '\\\n",
    "#         +'Climate modes which do not yield a higher predictive skill than a '\\\n",
    "#         +f'random variable are removed as follows {drop_dict}. The full list '\\\n",
    "#         +'of variables is as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, '\\\n",
    "#         +'NPI, NAM, NPO, PNA, NAO, SAM, TAS. The linear model is trained at '\\\n",
    "#         +f'different lag times of 1-20 years and for region {region_key} as'\\\n",
    "#         +'defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "#         +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "#         +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "#         +'learning rate = 5e-4.'\n",
    "\n",
    "#     region_weights.attrs = weights_attrs\n",
    "#     region_weights.to_netcdf(\n",
    "#         '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "#         +f'weights_linear_LEs_region_{region_key}_month_{month__}_'\\\n",
    "#         +f'dropped_useless_vars.nc'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4880a8-b81c-4c0d-9edb-584bb11df100",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Remove one variable at a time from the MMLE 3+ models, and retrain on the useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "342d543c-fd3b-45cc-99af-2577c601d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-10 09:54:57.682755 1\n",
      "2023-03-10 09:54:57.683428 2\n",
      "2023-03-10 09:54:57.683508 3\n",
      "2023-03-10 09:54:57.683550 4\n",
      "2023-03-10 09:54:57.683619 AMO_1\n",
      "2023-03-10 09:55:51.048695 AMO_4\n",
      "2023-03-10 09:56:44.936684 AMO_7\n",
      "2023-03-10 09:57:38.676356 AMO_10\n",
      "2023-03-10 09:58:32.428096 IPO_1\n",
      "2023-03-10 09:59:25.946433 NINO34_1\n",
      "2023-03-10 10:00:22.085388 NINO34_4\n",
      "2023-03-10 10:01:18.912533 NINO34_7\n",
      "2023-03-10 10:02:11.911358 NINO34_10\n",
      "2023-03-10 10:03:04.731529 PDO_1\n",
      "2023-03-10 10:03:57.569299 PDO_4\n",
      "2023-03-10 10:04:50.442948 PDO_7\n",
      "2023-03-10 10:05:43.267633 PDO_10\n",
      "2023-03-10 10:06:35.969264 AMM_1\n",
      "2023-03-10 10:07:28.636652 AMM_4\n",
      "2023-03-10 10:08:21.662317 AMM_7\n",
      "2023-03-10 10:09:15.293885 AMM_10\n",
      "2023-03-10 10:10:10.025805 ATN_1\n",
      "2023-03-10 10:11:03.525362 ATN_4\n",
      "2023-03-10 10:11:56.319964 ATN_7\n",
      "2023-03-10 10:12:50.402928 ATN_10\n",
      "2023-03-10 10:13:43.677340 IOD_1\n",
      "2023-03-10 10:14:40.468639 IOD_4\n",
      "2023-03-10 10:15:39.367538 IOD_7\n",
      "2023-03-10 10:16:38.130180 IOD_10\n",
      "2023-03-10 10:17:36.977863 NPI_1\n",
      "2023-03-10 10:18:36.079089 NAM_1\n",
      "2023-03-10 10:19:34.856843 NAM_4\n",
      "2023-03-10 10:20:33.478310 NAM_7\n",
      "2023-03-10 10:21:32.441678 NAM_10\n",
      "2023-03-10 10:22:31.208335 NPO_1\n",
      "2023-03-10 10:23:29.996025 NPO_4\n",
      "2023-03-10 10:24:29.011636 NPO_7\n",
      "2023-03-10 10:25:22.335210 NPO_10\n",
      "2023-03-10 10:26:14.276627 PNA_1\n",
      "2023-03-10 10:27:06.168670 PNA_4\n",
      "2023-03-10 10:27:58.588248 PNA_7\n",
      "2023-03-10 10:28:52.438221 PNA_10\n",
      "2023-03-10 10:29:49.164792 NAO_1\n",
      "2023-03-10 10:30:47.125884 NAO_4\n",
      "2023-03-10 10:31:46.301944 NAO_7\n",
      "2023-03-10 10:32:40.149314 NAO_10\n",
      "2023-03-10 10:33:35.590350 SAM_1\n",
      "2023-03-10 10:34:31.504091 SAM_4\n",
      "2023-03-10 10:35:26.555703 SAM_7\n",
      "2023-03-10 10:36:21.856629 SAM_10\n",
      "2023-03-10 10:39:07.253004 TAS_7\n",
      "2023-03-10 10:40:02.140498 TAS_10\n",
      "2023-03-10 10:40:59.988952 RAND_1\n",
      "2023-03-10 10:41:53.746124 5\n",
      "2023-03-10 10:41:53.746807 AMO_1\n",
      "2023-03-10 10:42:47.040519 AMO_4\n",
      "2023-03-10 10:43:40.501946 AMO_7\n",
      "2023-03-10 10:44:33.160256 AMO_10\n",
      "2023-03-10 10:45:26.044718 IPO_1\n",
      "2023-03-10 10:47:11.029321 NINO34_4\n",
      "2023-03-10 10:48:04.461799 NINO34_7\n",
      "2023-03-10 10:49:01.449160 NINO34_10\n",
      "2023-03-10 10:49:58.549877 PDO_1\n",
      "2023-03-10 10:50:52.326452 PDO_4\n",
      "2023-03-10 10:51:46.898572 PDO_7\n",
      "2023-03-10 10:52:40.324420 PDO_10\n",
      "2023-03-10 10:53:33.133519 AMM_1\n",
      "2023-03-10 10:54:26.214613 AMM_4\n",
      "2023-03-10 10:55:19.520808 AMM_7\n",
      "2023-03-10 10:56:14.354787 AMM_10\n",
      "2023-03-10 10:57:11.564743 ATN_1\n",
      "2023-03-10 10:58:08.547756 ATN_4\n",
      "2023-03-10 10:59:01.869338 ATN_7\n",
      "2023-03-10 11:00:52.159178 IOD_1\n",
      "2023-03-10 11:01:48.737454 IOD_4\n",
      "2023-03-10 11:02:43.614193 IOD_7\n",
      "2023-03-10 11:03:40.080722 IOD_10\n",
      "2023-03-10 11:04:36.780976 NPI_1\n",
      "2023-03-10 11:05:30.955390 NAM_1\n",
      "2023-03-10 11:06:24.240546 NAM_4\n",
      "2023-03-10 11:07:18.174213 NAM_7\n",
      "2023-03-10 11:08:15.013066 NAM_10\n",
      "2023-03-10 11:09:11.344730 NPO_1\n",
      "2023-03-10 11:10:11.853763 NPO_4\n",
      "2023-03-10 11:11:09.838930 NPO_7\n",
      "2023-03-10 11:12:07.020826 NPO_10\n",
      "2023-03-10 11:13:01.947999 PNA_1\n",
      "2023-03-10 11:13:55.450938 PNA_4\n",
      "2023-03-10 11:14:51.247013 PNA_7\n",
      "2023-03-10 11:15:46.797031 PNA_10\n",
      "2023-03-10 11:16:39.739846 NAO_1\n",
      "2023-03-10 11:17:32.441926 NAO_4\n",
      "2023-03-10 11:18:25.414533 NAO_7\n",
      "2023-03-10 11:19:18.331786 NAO_10\n",
      "2023-03-10 11:20:11.469657 SAM_1\n",
      "2023-03-10 11:21:07.513954 SAM_4\n",
      "2023-03-10 11:22:03.905559 SAM_7\n",
      "2023-03-10 11:22:59.118084 SAM_10\n",
      "2023-03-10 11:23:54.091168 TAS_1\n",
      "2023-03-10 11:24:50.719431 TAS_4\n",
      "2023-03-10 11:25:47.027817 TAS_7\n",
      "2023-03-10 11:26:42.591194 TAS_10\n",
      "2023-03-10 11:27:38.975842 RAND_1\n",
      "2023-03-10 11:28:34.730964 6\n",
      "2023-03-10 11:28:34.731591 AMO_1\n",
      "2023-03-10 11:29:27.695795 AMO_4\n",
      "2023-03-10 11:30:20.770259 AMO_7\n",
      "2023-03-10 11:31:13.668876 AMO_10\n",
      "2023-03-10 11:32:10.333488 IPO_1\n",
      "2023-03-10 11:33:07.673301 NINO34_1\n",
      "2023-03-10 11:34:04.825747 NINO34_4\n",
      "2023-03-10 11:35:01.949654 NINO34_7\n",
      "2023-03-10 11:35:54.839506 NINO34_10\n",
      "2023-03-10 11:36:50.656732 PDO_1\n",
      "2023-03-10 11:37:48.167213 PDO_4\n",
      "2023-03-10 11:38:45.724006 PDO_7\n",
      "2023-03-10 11:39:42.908959 PDO_10\n",
      "2023-03-10 11:40:37.027720 AMM_1\n",
      "2023-03-10 11:41:29.795337 AMM_4\n",
      "2023-03-10 11:42:22.573024 AMM_7\n",
      "2023-03-10 11:43:15.260347 AMM_10\n",
      "2023-03-10 11:44:07.975330 ATN_1\n",
      "2023-03-10 11:45:02.916344 ATN_4\n",
      "2023-03-10 11:45:59.585741 ATN_7\n",
      "2023-03-10 11:46:56.463849 ATN_10\n",
      "2023-03-10 11:47:53.691461 IOD_1\n",
      "2023-03-10 11:48:51.282759 IOD_4\n",
      "2023-03-10 11:49:48.643619 IOD_7\n",
      "2023-03-10 11:50:45.470963 IOD_10\n",
      "2023-03-10 11:51:38.609391 NPI_1\n",
      "2023-03-10 11:52:31.346678 NAM_1\n",
      "2023-03-10 11:53:24.180470 NAM_4\n",
      "2023-03-10 11:54:16.837256 NAM_7\n",
      "2023-03-10 11:55:10.277718 NAM_10\n",
      "2023-03-10 11:56:06.911301 NPO_1\n",
      "2023-03-10 11:57:04.045353 NPO_4\n",
      "2023-03-10 11:58:01.504409 NPO_7\n",
      "2023-03-10 11:58:59.148311 NPO_10\n",
      "2023-03-10 11:59:56.783958 PNA_1\n",
      "2023-03-10 12:00:54.452888 PNA_4\n",
      "2023-03-10 12:01:47.434714 PNA_7\n",
      "2023-03-10 12:02:40.065340 PNA_10\n",
      "2023-03-10 12:03:32.796625 NAO_1\n",
      "2023-03-10 12:04:25.464018 NAO_4\n",
      "2023-03-10 12:05:18.217966 NAO_7\n",
      "2023-03-10 12:06:11.015326 NAO_10\n",
      "2023-03-10 12:07:03.779662 SAM_1\n",
      "2023-03-10 12:07:56.670675 SAM_4\n",
      "2023-03-10 12:09:42.532480 SAM_10\n",
      "2023-03-10 12:10:37.392426 TAS_1\n",
      "2023-03-10 12:11:31.881921 TAS_4\n",
      "2023-03-10 12:12:26.180887 TAS_7\n",
      "2023-03-10 12:13:19.825664 TAS_10\n",
      "2023-03-10 12:14:13.035722 RAND_1\n",
      "2023-03-10 12:15:05.573665 11\n",
      "2023-03-10 12:15:05.574496 AMO_1\n",
      "2023-03-10 12:15:58.343158 AMO_4\n",
      "2023-03-10 12:16:51.152216 AMO_7\n",
      "2023-03-10 12:17:44.267110 AMO_10\n",
      "2023-03-10 12:18:37.214099 IPO_1\n",
      "2023-03-10 12:19:29.899149 NINO34_1\n",
      "2023-03-10 12:20:22.669694 NINO34_4\n",
      "2023-03-10 12:21:15.446032 NINO34_7\n",
      "2023-03-10 12:22:08.114563 NINO34_10\n",
      "2023-03-10 12:23:00.815506 PDO_1\n",
      "2023-03-10 12:23:53.678394 PDO_4\n",
      "2023-03-10 12:26:31.958638 AMM_1\n",
      "2023-03-10 12:27:24.568039 AMM_4\n",
      "2023-03-10 12:28:17.299899 AMM_7\n",
      "2023-03-10 12:29:09.829235 AMM_10\n",
      "2023-03-10 12:30:02.705293 ATN_1\n",
      "2023-03-10 12:30:55.343236 ATN_4\n",
      "2023-03-10 12:31:48.013340 ATN_7\n",
      "2023-03-10 12:32:40.792935 ATN_10\n",
      "2023-03-10 12:33:33.739971 IOD_1\n",
      "2023-03-10 12:34:26.596158 IOD_4\n",
      "2023-03-10 12:35:19.412159 IOD_7\n",
      "2023-03-10 12:36:12.053278 IOD_10\n",
      "2023-03-10 12:37:04.746156 NPI_1\n",
      "2023-03-10 12:37:57.946959 NAM_1\n",
      "2023-03-10 12:38:51.207550 NAM_4\n",
      "2023-03-10 12:39:44.450310 NAM_7\n",
      "2023-03-10 12:40:37.469418 NAM_10\n",
      "2023-03-10 12:43:16.009141 NPO_7\n",
      "2023-03-10 12:44:08.733714 NPO_10\n",
      "2023-03-10 12:45:01.373872 PNA_1\n",
      "2023-03-10 12:45:54.174965 PNA_4\n",
      "2023-03-10 12:46:46.852320 PNA_7\n",
      "2023-03-10 12:47:39.693303 PNA_10\n",
      "2023-03-10 12:48:32.651860 NAO_1\n",
      "2023-03-10 12:49:25.741161 NAO_4\n",
      "2023-03-10 12:50:18.476604 NAO_7\n",
      "2023-03-10 12:51:11.049379 NAO_10\n",
      "2023-03-10 12:52:03.652433 SAM_1\n",
      "2023-03-10 12:52:56.253699 SAM_4\n",
      "2023-03-10 12:53:48.842522 SAM_7\n",
      "2023-03-10 12:54:41.343589 SAM_10\n",
      "2023-03-10 12:55:33.937109 TAS_1\n",
      "2023-03-10 12:56:26.572040 TAS_4\n",
      "2023-03-10 12:57:19.280799 TAS_7\n",
      "2023-03-10 12:59:04.504720 RAND_1\n"
     ]
    }
   ],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "\n",
    "var_list_use = np.delete(np.array(var_month_list[:-3]).copy(),[5,6,7,29,30,31])\n",
    "\n",
    "for region_key in [1,2,3,4,5,6,11]:\n",
    "    print(datetime.datetime.now(), region_key)\n",
    "    \n",
    "    month__ = good_GCM_months[str(region_key)][0]\n",
    "    \n",
    "    #skip this one if already run this analysis\n",
    "    if len(glob.glob(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_linear_CMIP6_month_{str(month__).zfill(2)}_'\\\n",
    "        +f'var_15_drop_1_seasonal_region_{region_key}.nc')):\n",
    "        continue\n",
    "\n",
    "    r_values_list = []\n",
    "    weights_list  = []\n",
    "\n",
    "    for drop_var in var_list_use:\n",
    "        print(datetime.datetime.now(), drop_var)\n",
    "        \n",
    "        if drop_var == 'RAND_1':\n",
    "            r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                'CMIP6', month_=month__, region_list=[region_key], \n",
    "                lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                n_epoch=2000, learn_rate=5e-4, extra_drop_=None,\n",
    "                white_noise_=None,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                'CMIP6', month_=month__, region_list=[region_key], \n",
    "                lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                n_epoch=2000, learn_rate=5e-4, extra_drop_=[drop_var],\n",
    "                white_noise_=True,\n",
    "            )    \n",
    "\n",
    "        r_values_list.append(r_values_xr)\n",
    "        weights_list.append(all_weights_xr)\n",
    "        \n",
    "    r_values_save = xr.concat((r_values_list), dim='drop_var')\n",
    "    r_values_save['drop_var'] = var_list_use\n",
    "    \n",
    "    weights_save = xr.concat((weights_list), dim='drop_var')\n",
    "    weights_save['drop_var'] = var_list_use\n",
    "\n",
    "    r_values_attrs = {\n",
    "        'Description': 'Pearson correlation coefficient for validation '\\\n",
    "            +'data of the second members from 42 CMIP6 GCMs. Training data '\\\n",
    "            +'was the first member. 14 of the 15 modes of variability as '\\\n",
    "            +'computed by the Climate Variability Diagnostics Package (CVDP) '\\\n",
    "            +'with 2 year lowpass filtered sea ice concentration. These '\\\n",
    "            +'climate modes are as follows: AMO, IPO, NINO34, PDO, AMM, ATN, '\\\n",
    "            +'IOD, NPI, NAM, NPO, PNA, NAO, SAM, TAS, RAND. The linear model '\\\n",
    "            +'is trained at different lag times of 1-20 years and for each '\\\n",
    "            +'region (regions are defined by NSIDC MASIE-NH Version 1 '\\\n",
    "            +'(doi:10.7265/N5GT5K3K). Using an L1 loss function and Adam '\\\n",
    "            +'optimizer with 2000 epochs: nn.Sequential(nn.Linear(51,1,'\\\n",
    "            +'bias=False)), learning rate = 5e-4.',\n",
    "        'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "            \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "        'Data source': f'CMIP6 global climate model multi-model ensemble'\\\n",
    "            +f'doi:10.5194/gmd-9-1937-2016 for historical sea ice '\\\n",
    "            +'concentration simulation, climate modes calculated by CVDP '\\\n",
    "            +'(doi:10.1002/2014EO490002)',\n",
    "        'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "            +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "    }\n",
    "\n",
    "    r_values_save.attrs = r_values_attrs\n",
    "    r_values_save.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_linear_CMIP6_month_{str(month__).zfill(2)}_'\\\n",
    "        +f'var_15_drop_1_seasonal_region_{region_key}.nc'\n",
    "    )\n",
    "\n",
    "\n",
    "    weights_attrs = r_values_attrs.copy()\n",
    "    weights_attrs['Description'] = 'Linear weights including 14 of the 15 '\\\n",
    "        +'modes of variability as computed by the Climate Variability '\\\n",
    "        +'Diagnostics Package (CVDP) with 2 year lowpass filtered sea ice '\\\n",
    "        +'concentration. The linear model is trained on the 1st member of 42 '\\\n",
    "        +'CMIP6 GCMs and validated on the second members. The climate modes '\\\n",
    "        +'are as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "        +'NPO, PNA, NAO, SAM, TAS, RAND. The linear model is trained at '\\\n",
    "        +'different lag times of 1-20 years and for each region (regions are '\\\n",
    "        +'defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). Using '\\\n",
    "        +'an L1 loss function and Adam optimizer with 2000 epochs: '\\\n",
    "        +'nn.Sequential(nn.Linear(51,1,bias=False)), learning rate = 5e-4.'\n",
    "\n",
    "    weights_save.attrs = weights_attrs\n",
    "    weights_save.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_CMIP6_month_{str(month__).zfill(2)}_'\\\n",
    "        +f'var_15_drop_1_seasonal_region_{region_key}.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679849b-9b2e-471b-bf6a-9b2561be4971",
   "metadata": {},
   "source": [
    "## Retrain the CMIP6 linear model with only the useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "875dc7cd-1eee-434d-834c-c0c203eadb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all of the drop1 CMIP6 data\n",
    "r_vals_CMIP6_drop1 = []\n",
    "for month_ in np.arange(1,13):\n",
    "\n",
    "    r_values = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_linear_CMIP6_month_{str(month_).zfill(2)}'\\\n",
    "        +f'_var_15_drop_1.nc'\n",
    "    )\n",
    "\n",
    "    r_vals_CMIP6_drop1.append(r_values['r_value'])\n",
    "    \n",
    "r_vals_CMIP6_drop1 = xr.concat((r_vals_CMIP6_drop1), dim='month')\n",
    "r_vals_CMIP6_drop1['month'] = np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8c5f76c-652e-45df-af7d-40daa5f78faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#obtain all of the train/validate/test GCM members\n",
    "CMIP6_GCM_list = []\n",
    "for GCM in np.sort(list(good_GCM_mem.keys())):    \n",
    "    n_mem = len(good_GCM_mem[GCM])\n",
    "    if n_mem > 2:\n",
    "        CMIP6_GCM_list.append(GCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f7d24dc-6bf6-4700-84e1-4a84468a42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "lag_range = np.arange(5,11)\n",
    "\n",
    "month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "               'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "#select the months where multi-model ensemble has highest r2 above persistence\n",
    "good_CMIP6_months = [9, 8, 10, 10, 10, 9, 8]\n",
    "\n",
    "CMIP6_r_vals  = []\n",
    "CMIP6_weights = []\n",
    "for region_i, region_key in enumerate([1,2,3,4,5,6,11]):\n",
    "    print(datetime.datetime.now(), region_key)\n",
    "\n",
    "    month__ = good_CMIP6_months[region_i]\n",
    "\n",
    "    r_diff = r_vals_CMIP6_drop1.sel(region=region_key).sel(month=month__).sel(\n",
    "        lag=lag_range).mean('lag')**2 - r_vals_CMIP6_drop1.sel(\n",
    "        region=region_key).sel(month=month__).sel(lag=lag_range).mean(\n",
    "        'lag').sel(drop_var='RAND')**2\n",
    "\n",
    "    drop_list = r_diff['drop_var'].where(r_diff>0, drop=True).values\n",
    "    drop_dict[str(region_)] = drop_list\n",
    "    print(datetime.datetime.now(), drop_list)\n",
    "\n",
    "    r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "        model_name='CMIP6', month_=month__, region_list=[region_key], \n",
    "        lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "        n_epoch=2000, learn_rate=5e-4, extra_drop_=drop_list,\n",
    "        white_noise_=False,\n",
    "    )  \n",
    "\n",
    "    CMIP6_r_vals.append(r_values_xr)\n",
    "    CMIP6_weights.append(all_weights_xr)\n",
    "        \n",
    "region_r_vals = xr.concat((CMIP6_r_vals), dim='region')\n",
    "region_r_vals['region'] = [1,2,3,4,5,6,11]\n",
    "\n",
    "region_weights = xr.concat((CMIP6_weights), dim='region')\n",
    "region_weights['region'] = [1,2,3,4,5,6,11]\n",
    "\n",
    "r_values_attrs = {\n",
    "    'Description': 'Pearson correlation coefficient for validation '\\\n",
    "        +'data from all second ensemble members from 42 CMIP6 Global Climate '\\\n",
    "        +f'Models, train on first members. The full list of GMCs is as '\\\n",
    "        +f'follows: {CMIP6_GCM_list}, for regions [1,2,3,4,5,6,11] with '\\\n",
    "        +f'months corresponding to {good_GCM_months}. The modes of '\\\n",
    "        +f'variability yielding lower predictive skill than a random variable '\\\n",
    "        +f'are dropped as follows: {drop_dict}. The full list of 15 '\\\n",
    "        +'variables is: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "        +'NPO, PNA, NAO, SAM, TAS, as computed by the Climate Variability '\\\n",
    "        +'Diagnostics Package (CVDP) with 2 year lowpass filtered '\\\n",
    "        +'sea ice concentration. The linear model is trained at '\\\n",
    "        +f'different lag times of 1-20 years, the regions are defined '\\\n",
    "        +'by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "        +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "        +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "        +'learning rate = 5e-4.',\n",
    "    'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': f'CMIP6 global climate model historical simulations '\\\n",
    "        +'for historical sea ice concentration, with climate modes '\\\n",
    "        +'calculated by CVDP (doi:10.1002/2014EO490002)',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "        +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "\n",
    "region_r_vals.attrs = r_values_attrs\n",
    "region_r_vals.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    +f'validation_r_values_linear_CMIP6_all_regions_best_months_'\\\n",
    "    +f'dropped_useless_vars.nc'\n",
    ")\n",
    "\n",
    "\n",
    "weights_attrs = r_values_attrs.copy()\n",
    "weights_attrs['Description'] = 'Linear weights of the useful modes of '\\\n",
    "    +'variability as computed by the Climate Variability Diagnoistics'\\\n",
    "    +'Package (CVDP) with 2 year lowpass filtered sea ice concentration '\\\n",
    "    +f'for all regions [1,2,3,4,5,6,11] for months {good_GCM_months}. '\\\n",
    "    +f'The linear model is trained/validated on the first/second of all '\\\n",
    "    +f'members from the global climate models as follows {CMIP6_GCM_list}. '\\\n",
    "    +'Climate modes which do not yield a higher predictive skill than a '\\\n",
    "    +f'random variable are removed as follows {drop_dict}. The full list '\\\n",
    "    +'of variables is as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, '\\\n",
    "    +'NPI, NAM, NPO, PNA, NAO, SAM, TAS. The linear model is trained at '\\\n",
    "    +f'different lag times of 1-20 years and for region {region_key} as'\\\n",
    "    +'defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "    +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "    +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "    +'learning rate = 5e-4.'\n",
    "\n",
    "region_weights.attrs = weights_attrs\n",
    "region_weights.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    +f'weights_linear_CMIP6_all_regions_best_months_'\\\n",
    "    +f'dropped_useless_vars.nc'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL-3.7.9",
   "language": "python",
   "name": "npl-3.7.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
