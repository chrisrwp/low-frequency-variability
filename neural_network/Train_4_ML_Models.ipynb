{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a23acf-88ff-43a3-8771-dbae0ae876f4",
   "metadata": {},
   "source": [
    "# Train four different ML models with PyTorch to test linearity and independence\n",
    "\n",
    "1. Train four different ML algorithums on all CMIP6 Global Climate Model large ensembles (LEs) with >15 members\n",
    "2. Train four different ML algorithums on the multi-model CMIP6 ensemble (n=41) with 1st/2nd/3rd members as training/validation/test members\n",
    "3. Remove one variable at a time from the linear model, trained on the first 75% of the LE members\n",
    "4. Remove one variable at a time from the linear model, trained on the CMIP6 1st members  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c11f52-05dd-4337-bcf0-87b080c774a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-13 11:02:37.916563\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import dask\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def33401-b76d-44d8-89d8-75f373f374f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0); #for reproducability\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6157630-a522-4454-b69a-095b7bade1f6",
   "metadata": {},
   "source": [
    "## Load data and define useful functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f82f6403-fcee-4d26-8f50-d7482c54951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load useful data such as the GCM names and their DOIs\n",
    "CMIP6_info = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/raw_data/CMIP6_info/'\\\n",
    "    +'CMIP6_modeling_center_members_doi.nc'\n",
    ")\n",
    "\n",
    "#loop through all GCMs and make list of all realizations common to regional\n",
    "#SIC data and all CVDP variables\n",
    "good_GCM_mem = {}\n",
    "for GCM in CMIP6_info['model'].values:\n",
    "    try:\n",
    "        #open the regional SIC file and list the members, all members in file have\n",
    "        #already gone through quality control to check no nan or 0 values\n",
    "        SIC = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "            +f'Regional_SIC_detrended_lowpass_filter_{GCM}_1920_2014.nc')\n",
    "        SIC_mems = SIC['member'].values\n",
    "\n",
    "        #open the CVDP data and list all of the members which do not have nan values\n",
    "        #for the AMO - if they do have nan values do not use list that member.\n",
    "        CVDP = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "            +f'CVDP_standardized_linear_detrended_1920_2014_historical_{GCM}.nc')\n",
    "        CVDP_mems = CVDP['member'].where(\n",
    "            ~xr.ufuncs.isnan(CVDP['AMO']).max('time'), drop=True)\n",
    "\n",
    "        #now list the members with both good SIC and CVDP data, if at least 1\n",
    "        if len(np.intersect1d(CVDP_mems, SIC_mems)) > 0:\n",
    "            good_GCM_mem[GCM] = np.sort(np.intersect1d(CVDP_mems, SIC_mems))\n",
    "        else:\n",
    "            print(GCM, 'No members with good SIC and CVDP data')\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(GCM, 'Either SIC or CVDP data missing')\n",
    "    \n",
    "#remove CESM2 from the dictionary as replace with CESM2-LENS data\n",
    "good_GCM_mem.pop('CESM2', None);\n",
    "\n",
    "#save the member data\n",
    "# with open('/glade/work/cwpowell/low-frequency-variability/raw_data/CMIP6_info/'\\\n",
    "#           +'CMIP6_members_CVDP_and_SIC.pickle', 'wb') as handle:\n",
    "#     pickle.dump(good_GCM_mem, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7649a79-ea17-4518-9692-f595f2ce6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the train/validation/test split for the large ensembles with at least 15\n",
    "#members\n",
    "train_valid_test = [0.75, 0.15, 0.10]\n",
    "\n",
    "LE_train_mem = {}\n",
    "LE_valid_mem = {}\n",
    "LE_test_mem  = {}\n",
    "LE_GCM_list = []\n",
    "for GCM in np.sort(list(good_GCM_mem.keys())):\n",
    "    \n",
    "    n_mem = len(good_GCM_mem[GCM])\n",
    "    \n",
    "    if n_mem > 15:\n",
    "        LE_GCM_list.append(GCM)\n",
    "        train_n = int(np.ceil(n_mem*train_valid_test[0]))\n",
    "        test_n  = int(np.floor(n_mem*train_valid_test[2]))\n",
    "\n",
    "        LE_train_mem[GCM] = good_GCM_mem[GCM][:train_n]\n",
    "        LE_valid_mem[GCM] = good_GCM_mem[GCM][train_n:-test_n]\n",
    "        LE_test_mem[GCM]  = good_GCM_mem[GCM][-test_n:]\n",
    "        \n",
    "LE_GCM_list = np.sort(LE_GCM_list)\n",
    "\n",
    "#add in the CMIP6 member number codes\n",
    "CVDP_CMIP6_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'CVDP_standardized_linear_detrended_1920_2014_historical_CMIP6.nc')\n",
    "\n",
    "SIC_CMIP6_xr = xr.open_dataset(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'Regional_SIC_detrended_lowpass_filter_CMIP6_1920_2014.nc')\n",
    "\n",
    "LE_train_mem['CMIP6'] = CVDP_CMIP6_xr['member'].sel(\n",
    "    member=slice(1000,1999)).values\n",
    "LE_valid_mem['CMIP6'] = CVDP_CMIP6_xr['member'].sel(\n",
    "    member=slice(2000,2999)).values\n",
    "LE_test_mem['CMIP6']  = CVDP_CMIP6_xr['member'].sel(\n",
    "    member=slice(10000,None)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4037d758-6506-4b03-a117-2c801830e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a list of variable and season names\n",
    "CVDP_sample = xr.open_dataset('/glade/work/cwpowell/low-frequency-variability/'\\\n",
    "    +'input_data/CVDP_standardized_linear_detrended_1920_'\\\n",
    "    +'2014_historical_CanESM5.nc')\n",
    "\n",
    "var_month_list = []\n",
    "for i in CVDP_sample.to_array()['variable'].drop_sel(\n",
    "    variable=['AMOC','NINO12','NINO3','NINO4']).values:\n",
    "    for month_num in [1,4,7,10]:\n",
    "        var_month_list.append(str(i)+'_'+str(month_num))\n",
    "        \n",
    "var_month_list.append('RAND_1')\n",
    "var_month_list.append('RAND_4')\n",
    "var_month_list.append('RAND_7')\n",
    "var_month_list.append('RAND_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00412afa-5e43-4e59-8367-2ff45564c940",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define functions for loading feature and target data, as well as training the 4 ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4b133e-ce35-4076-8528-a2f1624b6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CVDP(model_name, month_, lag_, start_end_yr, extra_drop=None, \n",
    "              white_noise=None):\n",
    "    '''\n",
    "    Load the training, validation and testing data of the climate modes of\n",
    "    varaibility (features), corresponding to the time period of the sea ice\n",
    "    data (target) and the lag time. Additionally, remove certain climate modes\n",
    "    and/or include a white noise variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The month of the target sea ice concentration, this will determine\n",
    "        how many years lagged each season is, e.g. if sea ice concentration\n",
    "        is for October the seasonal CVDP data will be lagged 2 years for DJF,\n",
    "        MAM and JJA, but 3 years for SON.\n",
    "    lag_: int,\n",
    "        The number of years the CVDP data is offset before the sea ice data \n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (insclusive) for the sea ice concentration data.\n",
    "    extra_drop: none or list of strings,\n",
    "        If False, no additonal variables are dropped. If a list of a string or\n",
    "        strings, those variables listed will be removed from the CVDP data. \n",
    "    white_noise: bool,\n",
    "        If True, add a variable of normalized random values in a gaussian \n",
    "        distribution for the 4 seasons, all members and all years.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    CVDP_train: PyTorch tensor,\n",
    "        The stacked CVDP for the training members, with shape \n",
    "        ([member x year],[variable x month]) e.g for the first 75% of the 65\n",
    "        CanESM5 members used for training: ([49x74],[17x4]) = (3626,68).\n",
    "    CVDP_valid: PyTorch tensor,\n",
    "        The stacked CVDP for the validation members, with shape\n",
    "        ([member x year],[variable x month]).\n",
    "    CVDP_test: PyTorch tensor,\n",
    "        The stacked CVDP for the testing members, with shape\n",
    "        ([member x year],[variable x month]).    \n",
    "    ''' \n",
    "    \n",
    "    #load CVDP features and convert to seasonal data\n",
    "    #select the NetCDF file with lowpass filtering or linear detrending\n",
    "    CVDP_year_month = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +'CVDP_standardized_linear_detrended_1920_2014_historical_'\\\n",
    "        +f'{model_name}.nc'\n",
    "    )    \n",
    "\n",
    "    CVDP_year_month = CVDP_year_month.to_array('variable').sortby('time')\n",
    "\n",
    "    month_seperate = []\n",
    "    for i in [1,4,7,10]:\n",
    "        temp_data = CVDP_year_month.sel(\n",
    "            time=CVDP_year_month['time.month']==i)\n",
    "        temp_data['time'] = np.arange(1920,2015)\n",
    "        month_seperate.append(temp_data)\n",
    "\n",
    "    CVDP_data = xr.concat((month_seperate), dim='month')\n",
    "    CVDP_data['month'] = [1,4,7,10]\n",
    "    CVDP_data = CVDP_data.rename({'time':'year'})\n",
    "\n",
    "    CVDP_data = CVDP_data.drop_sel(variable=['AMOC','NINO12','NINO3','NINO4'])\n",
    "    \n",
    "    if type(extra_drop) != type(None): #drop extra variables from the CVDP data\n",
    "        CVDP_data = CVDP_data.drop_sel(variable=extra_drop)\n",
    "        \n",
    "    #now stack the CVDP data into X members and years, Y features\n",
    "    CVDP_train = []\n",
    "    CVDP_test  = []\n",
    "    CVDP_valid = []\n",
    "    for lag_season in [1,4,7,10]: \n",
    "        if lag_season >= month_:\n",
    "            extra_year = 1\n",
    "        else:\n",
    "            extra_year = 0\n",
    "\n",
    "        CVDP_month_data = CVDP_data.sortby('member')\n",
    "            \n",
    "        CVDP_month_data = CVDP_month_data.sel(\n",
    "            month=lag_season).sel(\n",
    "            year=slice(str(start_end_yr[0]-lag_-extra_year), \n",
    "                       str(start_end_yr[1]-lag_-extra_year)))\n",
    "        CVDP_month_data['year'] = np.arange(\n",
    "            0,start_end_yr[1]-start_end_yr[0]+1)\n",
    "        \n",
    "        #now, optionally add in white noise as 4 seasons of a new variable\n",
    "        if white_noise:\n",
    "            white_noise_month = (\n",
    "                CVDP_month_data.copy().isel(variable=0) * 0 + np.random.normal(\n",
    "                    loc=0, scale=1, size=(len(CVDP_month_data['member']),\n",
    "                                          len(CVDP_month_data['year'])))\n",
    "            )\n",
    "\n",
    "            white_noise_month['variable'] = 'RAND'\n",
    "            \n",
    "            CVDP_month_data = xr.concat(\n",
    "                (CVDP_month_data, white_noise_month), dim='variable'\n",
    "            ) \n",
    "        \n",
    "        CVDP_train.append(CVDP_month_data.sel(member=LE_train_mem[model_name]))\n",
    "        CVDP_valid.append(CVDP_month_data.sel(member=LE_valid_mem[model_name]))\n",
    "        CVDP_test.append(CVDP_month_data.sel(member=LE_test_mem[model_name]))\n",
    "        \n",
    "        \n",
    "    CVDP_train_stacked = xr.concat((CVDP_train),'month').stack(\n",
    "        member_time=('member','year')).stack(\n",
    "        var_month=('variable','month'))\n",
    "    CVDP_train = torch.Tensor(CVDP_train_stacked.values)\n",
    "    \n",
    "    CVDP_valid_stacked = xr.concat((CVDP_valid),'month').stack(\n",
    "        member_time=('member','year')).stack(\n",
    "        var_month=('variable','month'))\n",
    "    CVDP_valid = torch.Tensor(CVDP_valid_stacked.values)\n",
    "\n",
    "    CVDP_test_stacked = xr.concat((CVDP_test),'month').stack(\n",
    "        member_time=('member','year')).stack(\n",
    "        var_month=('variable','month'))\n",
    "    CVDP_test = torch.Tensor(CVDP_test_stacked.values)\n",
    "    \n",
    "    return(CVDP_train, CVDP_valid, CVDP_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d28b73-ac61-454b-8534-9d04c33741a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_SIC(model_name, month_, region_, start_end_yr):\n",
    "    '''\n",
    "    Load the sea ice concentration anomalies (targets) for a specific GCM,\n",
    "    month and time period.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The month of the target sea ice concentration anomalies. \n",
    "    region_: int,\n",
    "        The region number for the sea ice concentration anomalies.\n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (inclusive) for the sea ice concentration \n",
    "        anomalies.\n",
    "    train_n: int,\n",
    "        The highest member index to include in training data.\n",
    "    test_n: int,\n",
    "        The index of the lowest member index included in the test data.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    target_train: PyTorch tensor,\n",
    "        The stacked sea ice concentration anomalies for the training members,\n",
    "        with shape (member, year) e.g for the first 75% of the 65 CanESM5\n",
    "        members used for training: (49,74). \n",
    "    target_valid: PyTorch tensor,\n",
    "        The stacked sea ice concentration anomalies for the validation members,\n",
    "        with shape (member, year).\n",
    "    target_test: PyTorch tensor,\n",
    "        The stacked sea ice concentration anomalies for the test members, with\n",
    "        shape (member, year).\n",
    "    \n",
    "    '''\n",
    "    #load SIC targets with the desired type of filtering/dretrending\n",
    "    SIC_data = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +f'Regional_SIC_detrended_lowpass_filter_{model_name}_1920_2014.nc'\n",
    "    )\n",
    "    \n",
    "    #select the years for this analysis period and sort by member\n",
    "    SIC_data = SIC_data['SIC'].sortby('member').sel(\n",
    "        year=slice(str(start_end_yr[0]), str(start_end_yr[1])))\n",
    "    \n",
    "    #convert 2D xr.DataArray to 1D xr.DataArray to np.ndarray to torch.Tensor\n",
    "    target_train = torch.from_numpy(\n",
    "        SIC_data.sel(member=LE_train_mem[model_name]).sel(month=month_).sel(\n",
    "            region=region_).stack(member_time=('member','year')).values\n",
    "    )\n",
    "    target_valid = torch.from_numpy(\n",
    "        SIC_data.sel(member=LE_valid_mem[model_name]).sel(month=month_).sel(\n",
    "            region=region_).stack(member_time=('member','year')).values\n",
    "    )\n",
    "    target_test = torch.from_numpy(\n",
    "        SIC_data.sel(member=LE_test_mem[model_name]).sel(month=month_).sel(\n",
    "            region=region_).stack(member_time=('member','year')).values\n",
    "    )\n",
    "    \n",
    "    return(target_train, target_valid, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd36884-be9c-45f9-af65-81eb299a2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_4_ML_for_LE(model_name, month_, region_list, lag_list, start_end_yr, \n",
    "                      n_epoch, learn_rates, white_noise_=None):\n",
    "    '''\n",
    "    Train the 4 machine learning models for a given large ensemble on a \n",
    "    specificed of sea ice concentration data for a specified set of months,\n",
    "    regions, and lags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The months of sea ice concentration anomalies on which to train the \n",
    "        machine learning model. \n",
    "    region_list: list of ints,\n",
    "        A list of the regional sea ice anomalies to train the model on \n",
    "        separately.\n",
    "    lag_list: list of ints,\n",
    "        The range of lagged year on which to separately train the machine \n",
    "        learning model \n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (inclusive) for the sea ice concentration data.\n",
    "    white_noise: bool,\n",
    "        If True, add a variable of normalized random values in a gaussian \n",
    "        distribution for the 4 seasons, all members and all years.\n",
    "    n_epoch: int, \n",
    "        number of epochs with which to train all of the machine learning \n",
    "        models.\n",
    "    learn_rates: list of ints,\n",
    "        List of 4 integers corresponding to the learning rate for each of the 4\n",
    "        machine learning algorithums.                      \n",
    "                      \n",
    "    Returns\n",
    "    ----------\n",
    "    r_values_xr: xarray.Dataset,\n",
    "        The pearson correlation coefficients for the 4 machine learning \n",
    "        algorithms from the validation data.\n",
    "    all_1_grads_xr: xarray.Dataset,\n",
    "        The gradients from the machine learning model using a simple multiple\n",
    "        linear regression model.    \n",
    "    '''\n",
    "\n",
    "    if white_noise_:\n",
    "        n_features = 15*4\n",
    "    else:\n",
    "        n_features = 14*4\n",
    "        \n",
    "    if model_name == 'CMIP6':\n",
    "        doi_model = '10.5194/gmd-9-1937-2016'\n",
    "    else:\n",
    "        doi_model = CMIP6_info['doi'].sel(model=model_name).values\n",
    "    \n",
    "    all_1_weights = np.empty(\n",
    "        [len(region_list), len(lag_list), n_features], dtype=float)\n",
    "\n",
    "    all_r_values = np.empty(\n",
    "        [len(region_list), len(lag_list), 4], dtype=float)\n",
    "\n",
    "    for region_i, region_ in enumerate(region_list):\n",
    "\n",
    "        for lag_i, lag_ in enumerate(lag_list):\n",
    "            #load the feature and target data for the correct model, month,\n",
    "            #region and lag\n",
    "            CVDP_train, CVDP_valid, CVDP_test = load_CVDP(\n",
    "                model_name, month_, lag_, start_end_yr, \n",
    "                extra_drop=None, white_noise=white_noise_\n",
    "            )\n",
    "\n",
    "            target_train, target_valid, target_test = load_SIC(\n",
    "                model_name, month_, region_, start_end_yr, \n",
    "            )\n",
    "\n",
    "            ML_model_computed = []\n",
    "            for ML_i in range(4):\n",
    "                if ML_i == 0:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,1,bias=False)\n",
    "                    )\n",
    "                elif ML_i == 1:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,1,bias=False), nn.ReLU()\n",
    "                    )\n",
    "                elif ML_i == 2:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,n_neurons), \n",
    "                        nn.Linear(n_neurons,n_neurons),\n",
    "                        nn.Linear(n_neurons,1)\n",
    "                    )\n",
    "                elif ML_i == 3:\n",
    "                    ML_model_use = nn.Sequential(\n",
    "                        nn.Linear(n_features,n_neurons), nn.ReLU(),\n",
    "                        nn.Linear(n_neurons,n_neurons), nn.ReLU(),\n",
    "                        nn.Linear(n_neurons,1)\n",
    "                    )\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    params=ML_model_use.parameters(), \n",
    "                    lr=learn_rates[ML_i]\n",
    "                )\n",
    "\n",
    "                ##### train the model #####\n",
    "                train_r, valid_r = [],[]\n",
    "                for epoch in range(n_epoch):\n",
    "                    # TRAIN\n",
    "                    prediction = ML_model_use(CVDP_train)\n",
    "                    optimizer.zero_grad() #reset the gradients to zeros\n",
    "                    loss = loss_fcn(prediction[:,0].double(), target_train)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_r.append(\n",
    "                        np.corrcoef(prediction[:,0].detach().numpy(),\n",
    "                                    target_train.detach().numpy()\n",
    "                                   )[1][0]\n",
    "                    )\n",
    "\n",
    "                    if ML_i == 0:\n",
    "                        ML_model_computed.append(ML_model_use)\n",
    "\n",
    "                    ##### validate the model #####\n",
    "                    with torch.no_grad():\n",
    "                        p_val = ML_model_use(CVDP_valid)\n",
    "                        loss_val = loss_fcn(p_val[:,0].double(), \n",
    "                                            target_valid\n",
    "                                           )\n",
    "\n",
    "                        valid_r.append(\n",
    "                            np.corrcoef(p_val[:,0].detach().numpy(),\n",
    "                                        target_valid.detach().numpy()\n",
    "                                       )[1][0]\n",
    "                        )\n",
    "\n",
    "                #select highest validation r value from all epochs\n",
    "                all_r_values[region_i][lag_i][ML_i] = np.max(valid_r)\n",
    "\n",
    "                if ML_i == 0: #record the weights of the best epoch\n",
    "                    all_1_weights[region_i][lag_i] = np.ravel(\n",
    "                        ML_model_computed[np.argmax(valid_r)][\n",
    "                            0].weight[0,:].detach().numpy())\n",
    "\n",
    "    if model_name == 'CMIP6':\n",
    "        model_name = 'CMIP6 multi-model large ensemble'\n",
    "    \n",
    "    #save the r values to NetCDF every model and month\n",
    "    r_values_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'r_value':(['region', 'lag', 'ML_model'], all_r_values)},\n",
    "        coords = {\n",
    "            'region':region_list, 'lag':lag_list, 'ML_model':range(4)},\n",
    "    )\n",
    "\n",
    "    r_values_xr_attrs = {\n",
    "        'Description': 'Pearson correlation coefficient for validation '\\\n",
    "            +'data (15%) of the availible members of the global climate '\\\n",
    "            +f'model {model_name}. 4 different machine learning models '\\\n",
    "            +'fit the 16 features of seasonal climate modes computed by '\\\n",
    "            +'the Climate Variability Diagnostics Package with 2 year '\\\n",
    "            +'lowpass filtered sea ice concentration. These climate modes are '\\\n",
    "            +f'as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "            +'NPO, PNA, NAO, SAM, TAS. The ML models are '\\\n",
    "            +'trained at different lag times of 1-20 years and for each '\\\n",
    "            +'region (regions are defined by NSIDC MASIE-NH Version 1 '\\\n",
    "            +'(doi:10.7265/N5GT5K3K). The ML_model dimension refers to '\\\n",
    "            +'the 4 different ML architectures with PyTorch all using L1 '\\\n",
    "            +f'loss function and Adam optimizer and {n_epoch} '\\\n",
    "            +'epochs:'+'\\n'+'1 - nn.Sequential'\\\n",
    "            +f'(nn.Linear({n_features},1,bias=False)), learning rate = '\\\n",
    "            +f'{learn_rates[0]}.'+'\\n'+'2 - nn.Sequential(nn.Linear('\\\n",
    "            +f'{n_features},1,bias=False), nn.ReLU()), learning rate = '\\\n",
    "            +f'{learn_rates[1]}.'+'\\n3 - nn.Sequential(nn.Linear('\\\n",
    "            +f'{n_features},{n_neurons}), nn.Linear({n_neurons},'\\\n",
    "            +f'{n_neurons}), nn.Linear({n_neurons},1)), learning rate = '\\\n",
    "            +f'{learn_rates[2]}.'+'\\n'+'4 - nn.Sequential(nn.Linear('\\\n",
    "            +f'{n_features},{n_neurons}), nn.ReLU(),nn.Linear({n_neurons}'\\\n",
    "            +f',{n_neurons}), nn.ReLU(), nn.Linear({n_neurons},1)), '\\\n",
    "            +f'learning rate {learn_rates[3]}.',\n",
    "        'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "            \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "        'Data source': f'CMIP6 global climate model {model_name}, '\\\n",
    "            +f'doi:{doi_model} sea ice concentration and climate modes '\\\n",
    "            +'calculated by the Climate Variability Diagnostics Package '\\\n",
    "            +'(doi:10.1002/2014EO490002)',\n",
    "        'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "            +'variability/blob/main/neural_network/4_PyTorch_model_'\\\n",
    "            +'configurations.ipynb',\n",
    "    }\n",
    "\n",
    "    r_values_xr.attrs = r_values_xr_attrs\n",
    "\n",
    "    #save the linear 1 layer neural network weights to NetCDF\n",
    "    if white_noise_:\n",
    "        var_month_use = np.array(var_month_list).copy()\n",
    "    else:\n",
    "        var_month_use = np.array(var_month_list[:-4]).copy()\n",
    "        \n",
    "    all_1_weights_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'weights':(['region', 'lag', 'mode_month'], all_1_weights),\n",
    "        },\n",
    "        coords = {'region':region_list, 'lag':lag_list,\n",
    "                  'mode_month':var_month_use,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    all_1_weights_xr_attrs = r_values_xr_attrs.copy()\n",
    "    all_1_weights_xr_attrs['Description'] = 'Weights of the '\\\n",
    "        +'linear model fit for the validation data (15%) of the availible '\\\n",
    "        +f'members of the global climate model {model_name}. {n_features} '\\\n",
    "        +'features of seasonal climate modes computed by the Climate '\\\n",
    "        +'Variability Diagnostics Package with 2 year lowpass filtered sea '\\\n",
    "        +'ice concentration. These climate modes are as follows: AMO, IPO, '\\\n",
    "        +'NINO34, PDO, AMM, ATN, IOD, NPI, NAM, NPO, PNA, NAO, SAM, TAS. '\\\n",
    "        +'The model is trained at different lag times '\\\n",
    "        +'of 1-20 years and for each region (regions are defined by NSIDC '\\\n",
    "        +'MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). The model uses '\\\n",
    "        +f'PyTorch with a L1 loss function, {n_epoch} epochs, Adam'\\\n",
    "        +f' optimizer and is defined by nn.Linear({n_features},1,'\\\n",
    "        +f'bias=False)), learning rate = {learn_rates[0]}.'\n",
    "\n",
    "    all_1_weights_xr.attrs = all_1_weights_xr_attrs\n",
    "        \n",
    "    return(r_values_xr, all_1_weights_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc478daa-f246-49e8-9cf3-e303512a9a9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Train the 4 ML models with the first 75% of LE members, tested on next 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b6af0b15-0351-466f-91df-177ad9d4238c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "n_neurons = 8\n",
    "\n",
    "for month__ in np.arange(11,13):\n",
    "    print(datetime.datetime.now(), month__)\n",
    "\n",
    "    for model_name in LE_GCM_list:\n",
    "        print(datetime.datetime.now(), model_name)\n",
    "\n",
    "        r_values_xr, all_1_weights_xr = train_4_ML_for_LE(\n",
    "            model_name, month_=month__, region_list=[1,2,3,4,5,6,11], \n",
    "            lag_list=np.arange(1,21), start_end_yr=[1941,2014], n_epoch=2000, \n",
    "            learn_rates=[5e-4, 5e-4, 1e-4, 2e-4], white_noise_=False,\n",
    "        )    \n",
    "\n",
    "        r_values_xr.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'validation_r_values_4ML_{model_name}_month_'\\\n",
    "            +f'{str(month__).zfill(2)}_var_14.nc')\n",
    "        all_1_weights_xr.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'weights_linear_{model_name}_month_{str(month__).zfill(2)}_'\\\n",
    "            +'var_14.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e82c4-7165-4429-a924-d805e807ccd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Train the 4 ML models with all CMIP6 GCMs, 1st/2nd/3rd+ members training/validation/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b8062-5819-491a-b4fd-0f54c586ee6b",
   "metadata": {},
   "source": [
    "### Make the CMIP6 CVDP and SIC files as if 'CMIP6' was a GCM name and each GCM member 1 was a different member "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "641fca58-f4ac-4797-ba59-bcaddcdc078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain all of the train/validate/test GCM members\n",
    "CMIP6_GCM_list = []\n",
    "for GCM in np.sort(list(good_GCM_mem.keys())):    \n",
    "    n_mem = len(good_GCM_mem[GCM])\n",
    "    if n_mem > 2:\n",
    "        CMIP6_GCM_list.append(GCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a567c995-ec84-4466-bbfa-063eac4e95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather all of the members together and save to NetCDF\n",
    "CVDP_CMIP6 = []\n",
    "SIC_CMIP6 = []\n",
    "\n",
    "train_mem_i = 1000\n",
    "valid_mem_i = 2000\n",
    "test_mem_i  = 10000\n",
    "\n",
    "for GCM in CMIP6_GCM_list:\n",
    "    #loop through and append the correct members to the train, validation \n",
    "    #and testing groups with the \n",
    "    CVDP_data = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +'CVDP_standardized_linear_detrended_1920_2014_historical_'\\\n",
    "        +f'{GCM}.nc'\n",
    "    )\n",
    "    CVDP_data = CVDP_data.sel(member=good_GCM_mem[GCM])\n",
    "    \n",
    "    new_mem_list = np.arange(test_mem_i, test_mem_i+len(CVDP_data['member'])-2)\n",
    "    new_mem_list = np.insert(new_mem_list, 0, [train_mem_i, valid_mem_i])\n",
    "    \n",
    "    CVDP_data['member'] = new_mem_list\n",
    "    CVDP_CMIP6.append(CVDP_data)\n",
    "    \n",
    "    #now do the same for the SIC data\n",
    "    SIC_data = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "        +f'Regional_SIC_detrended_lowpass_filter_{GCM}_1920_2014.nc'\n",
    "    )\n",
    "    SIC_data = SIC_data.sel(member=good_GCM_mem[GCM])\n",
    "    \n",
    "    SIC_data['member'] = new_mem_list   \n",
    "    SIC_CMIP6.append(SIC_data)\n",
    "    \n",
    "    #now increase the initial value of the training, validation, and test member\n",
    "    #element numbers\n",
    "    train_mem_i += 1\n",
    "    valid_mem_i += 1\n",
    "    test_mem_i = test_mem_i + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8653df52-8c18-426e-a15f-de0551ee5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this CMIP6 data to NetCDF and include metadata\n",
    "CVDP_CMIP6_xr = xr.concat((CVDP_CMIP6),dim='member').sortby('member')\n",
    "\n",
    "CVDP_CMIP6_xr.attrs = {\n",
    "    'Description' : 'Linearly detrended and standardized variables from the '\\\n",
    "        +'CVDP (Climate Variability Diagnostics Package) for all CMIP6 '\\\n",
    "        +'global climate models with at least 3 available members. Seasonal '\\\n",
    "        +'data for 1920-2014. The members can be decoded as follows: '\\\n",
    "        +'1000-1999 are the training members, 2000-2999 are the validation '\\\n",
    "        +'members, and 10000+ are the test members. The GCM is encoded as '\\\n",
    "        +'the last 2 digits for the training and validation members, and the '\\\n",
    "        +'first 2 digits +10 for the testing members. Note there is always '\\\n",
    "        +'1 member from each GCM for training and validation, and all '\\\n",
    "        +'remaining members are with the testing dataset. The GCM numbering '\\\n",
    "        +'referrs to the following: 0 ACCESS-CM2, 1 ACCESS-ESM1-5, '\\\n",
    "        +'2 BCC-CSM2-MR, 3 BCC-ESM1, 4 CAMS-CSM1-0, 5 CESM2-FV2, '\\\n",
    "        +'6 CESM2-LENS, 7 CESM2-WACCM, 8 CESM2-WACCM-FV2, 9 CIESM, '\\\n",
    "        +'10 CMCC-CM2-SR5, 11 CNRM-CM6-1, 12 CNRM-ESM2-1, 13 CanESM5, '\\\n",
    "        +'14 CanESM5-CanOE, 15 E3SM-1-0, 16 EC-Earth3, 17 EC-Earth3-CC, '\\\n",
    "        +'18 EC-Earth3-Veg, 19 EC-Earth3-Veg-LR, 20 FIO-ESM-2-0, '\\\n",
    "        +'21 GFDL-ESM4, 22 GISS-E2-1-G, 23 GISS-E2-1-H, 24 GISS-E2-2-G, '\\\n",
    "        +'25 GISS-E2-2-H, 26 HadGEM3-GC31-LL, 27 HadGEM3-GC31-MM, '\\\n",
    "        +'28 INM-CM5-0, 29 IPSL-CM6A-LR, 30 MIROC-ES2H, 31 MIROC-ES2L, '\\\n",
    "        +'32 MIROC6, 33 MPI-ESM-1-2-HAM, 34 MPI-ESM1-2-HR, 35 MPI-ESM1-2-LR, '\\\n",
    "        +'36 MRI-ESM2-0, 37 NESM3, 38 NorCPM1, 39 NorESM2-LM, 40 NorESM2-MM, '\\\n",
    "        +'41 UKESM1-0-LL. All members are sorted alphabetically before '\\\n",
    "        +'being divided into the three groups',\n",
    "    'Units' :'standardized values',\n",
    "    'Timestamp' : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': 'CMIP6 historical simulations, computed by CVDP '\\\n",
    "        +'doi: 10.1002/2014EO490002.',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-fequency-variability/'\\\n",
    "            +'neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "    \n",
    "CVDP_CMIP6_xr.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'CVDP_standardized_linear_detrended_1920_2014_historical_CMIP6.nc')\n",
    "\n",
    "SIC_CMIP6_xr = xr.concat((SIC_CMIP6),dim='member').sortby('member')\n",
    "SIC_CMIP6_xr.attrs = {\n",
    "    'Description' : '2 year lowpass filter of linearly detrended regional '\\\n",
    "        +'average sea ice concentration (SIC) in % for the climate model for '\\\n",
    "        +'all CMIP6 global climate models with at least 3 available members. '\\\n",
    "        +'Seasonal data for 1920-2014. The members can be decoded as follows: '\\\n",
    "        +'1000-1999 are the training members, 2000-2999 are the validation '\\\n",
    "        +'members, and 10000+ are the test members. The GCM is encoded as '\\\n",
    "        +'the last 2 digits for the training and validation members, and the '\\\n",
    "        +'first 2 digits +10 for the testing members. Note there is always '\\\n",
    "        +'1 member from each GCM for training and validation, and all '\\\n",
    "        +'remaining members are with the testing dataset. The GCM numbering '\\\n",
    "        +'referrs to the following: 0 ACCESS-CM2, 1 ACCESS-ESM1-5, '\\\n",
    "        +'2 BCC-CSM2-MR, 3 BCC-ESM1, 4 CAMS-CSM1-0, 5 CESM2-FV2, '\\\n",
    "        +'6 CESM2-LENS, 7 CESM2-WACCM, 8 CESM2-WACCM-FV2, 9 CIESM, '\\\n",
    "        +'10 CMCC-CM2-SR5, 11 CNRM-CM6-1, 12 CNRM-ESM2-1, 13 CanESM5, '\\\n",
    "        +'14 CanESM5-CanOE, 15 E3SM-1-0, 16 EC-Earth3, 17 EC-Earth3-CC, '\\\n",
    "        +'18 EC-Earth3-Veg, 19 EC-Earth3-Veg-LR, 20 FIO-ESM-2-0, '\\\n",
    "        +'21 GFDL-ESM4, 22 GISS-E2-1-G, 23 GISS-E2-1-H, 24 GISS-E2-2-G,'\\\n",
    "        +'25 GISS-E2-2-H, 26 HadGEM3-GC31-LL, 27 HadGEM3-GC31-MM, '\\\n",
    "        +'28 INM-CM5-0, 29 IPSL-CM6A-LR, 30 MIROC-ES2H, 31 MIROC-ES2L, '\\\n",
    "        +'32 MIROC6, 33 MPI-ESM-1-2-HAM, 34 MPI-ESM1-2-HR, 35 MPI-ESM1-2-LR, '\\\n",
    "        +'36 MRI-ESM2-0, 37 NESM3, 38 NorCPM1, 39 NorESM2-LM, 40 NorESM2-MM, '\\\n",
    "        +'41 UKESM1-0-LL. All members are sorted alphabetically before '\\\n",
    "        +'being divided into the three groups',\n",
    "    'Timestamp' : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': 'CMIP6 historical simulations',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-fequency-variability/'\\\n",
    "            +'neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "    \n",
    "SIC_CMIP6_xr.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/input_data/'\\\n",
    "    +'Regional_SIC_detrended_lowpass_filter_CMIP6_1920_2014.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03506a0-d1d0-436d-9813-497973201f5f",
   "metadata": {},
   "source": [
    "## Now train the 4 ML models on the CMIP6 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c27196fa-dd96-49c8-b5da-c738b599734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "n_neurons = 8\n",
    "\n",
    "for month__ in np.arange(1,13):\n",
    "    print(datetime.datetime.now(), month__)\n",
    "\n",
    "    r_values_xr, all_1_weights_xr = train_4_ML_for_LE(\n",
    "        'CMIP6', month_=month__, region_list=[1,2,3,4,5,6,11], \n",
    "        lag_list=np.arange(1,21), start_end_yr=[1941,2014], n_epoch=2000, \n",
    "        learn_rates=[5e-4, 5e-4, 1e-4, 2e-4], white_noise_=False,\n",
    "    )    \n",
    "\n",
    "    r_values_xr.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_4ML_CMIP6_month_'\\\n",
    "        +f'{str(month__).zfill(2)}_var_14.nc')\n",
    "    all_1_weights_xr.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_CMIP6_month_{str(month__).zfill(2)}_'\\\n",
    "        +'var_14.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cd28e-3c11-4c67-9d84-ce73762404d0",
   "metadata": {},
   "source": [
    "## Test the CMIP6-trained linear model with 3rd members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf128d9-7bb3-4f18-9286-cff19fa8c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weight data \n",
    "CMIP6_weights = []\n",
    "for month_ in np.arange(1,13):\n",
    "    all_1_weights_xr = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_CMIP6_month_{str(month__).zfill(2)}_'\\\n",
    "        +'var_14.nc'\n",
    "    )\n",
    "    CMIP6_weights.append(all_1_weights_xr['weights'])\n",
    "\n",
    "CMIP6_weights = xr.concat((CMIP6_weights), dim='month')\n",
    "CMIP6_weights['month'] = np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "044c1a09-26d5-458f-bd73-85b4d285f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_r_vals = []\n",
    "for month_ in np.arange(1,13):\n",
    "    print(datetime.datetime.now(), month_)\n",
    "    lag_r_vals = []\n",
    "    for lag_ in np.arange(1,21):\n",
    "\n",
    "        CVDP_train, CVDP_valid, CVDP_test = load_CVDP(\n",
    "            'CMIP6', month_, lag_, [1941,2014], \n",
    "            extra_drop=None, white_noise=False,\n",
    "        )\n",
    "                \n",
    "        region_r_vals = []\n",
    "        for region_ in [1,2,3,4,5,6,11]:\n",
    "            target_train, target_valid, target_test = load_SIC(\n",
    "                 'CMIP6', month_, region_, [1941,2014], \n",
    "            )\n",
    "            \n",
    "            prediction = np.sum(\n",
    "                np.array(CVDP_test) \n",
    "                * np.tile(CMIP6_weights.sel(month=month_).sel(\n",
    "                    region=region_).sel(lag=lag_), \n",
    "                [38850,1]), axis=1\n",
    "            )\n",
    "            \n",
    "            all_test_mem = []\n",
    "            for i in range(525):\n",
    "                all_test_mem.append(\n",
    "                    np.corrcoef(prediction[i*74:(i+1)*74], \n",
    "                                target_test[i*74:(i+1)*74])[0][1])\n",
    "                \n",
    "            region_r_vals.append(all_test_mem)\n",
    "            \n",
    "        lag_r_vals.append(region_r_vals)\n",
    "        \n",
    "    month_r_vals.append(lag_r_vals)\n",
    "    \n",
    "month_r_vals_xr = xr.Dataset(\n",
    "    data_vars = {'r_value':(['month','lag','region','member'], month_r_vals)},\n",
    "    coords = {'month':np.arange(1,13), 'lag':np.arange(1,21), \n",
    "              'region':[1,2,3,4,5,6,11], \n",
    "              'member':SIC_CMIP6_xr['member'][-525:].values,\n",
    "             },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b60cdb8-d10c-48e0-81fb-a6e1e71900b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the r values and group by GCM\n",
    "r_val_by_GCM = []\n",
    "for GCM_i, GCM in enumerate(CMIP6_GCM_list):\n",
    "    begin_mem = str(10000+(GCM_i*1000))\n",
    "    end_mem   = str(10999+(GCM_i*1000))\n",
    "    \n",
    "    data = month_r_vals_xr.sel(member=slice(begin_mem,end_mem))\n",
    "    data['member'] = np.arange(0,len(data['member']))\n",
    "    r_val_by_GCM.append(data)\n",
    "\n",
    "r_val_by_GCM = xr.concat((r_val_by_GCM), dim='model_name')\n",
    "r_val_by_GCM['model_name'] = CMIP6_GCM_list\n",
    "\n",
    "r_val_by_GCM.attrs = {\n",
    "    'Description': 'Pearson correlation coefficient for test data (3rd and '\\\n",
    "        +'later members of 41 global climate models as follows: '\\\n",
    "        +f'{CMIP6_GCM_list}. Model train and validated on the 1st and 2nd '\\\n",
    "        +'members of the same 41 GCMs. The model features were the following '\\\n",
    "        +'modes of variability: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, '\\\n",
    "        +'NAM, NPO, PNA, NAO, SAM, TAS. The targets were 2 year lowpass '\\\n",
    "        +'filtered regional Arctic sea ice concentration anomalies for lag '\\\n",
    "        +'times of 1-20 years. Regions are defined by NSIDC MASIE-NH Version'\\\n",
    "        +' 1 (doi:10.7265/N5GT5K3K). The climate modes are computed by the '\\\n",
    "        +'Climate Variability Diagnostics Package (CVDP). The model was '\\\n",
    "        +'trained using an L1 loss function and Adam optimizer with 2000 '\\\n",
    "        +'epochs: nn.Sequential(nn.Linear(56,1,bias=False)), with a '\\\n",
    "        +'learning rate of 5e-4.',\n",
    "    'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': f'CMIP6 global climate models for historical simulations '\\\n",
    "        +'with sea ice concentration output, climate modes calculated by CVDP '\\\n",
    "        +'(doi:10.1002/2014EO490002)',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "        +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "\n",
    "r_val_by_GCM.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    +f'test_r_values_linear_CMIP6_var_15_all_months_all_regions.nc'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d95d9-0051-4fa9-b30e-b7d3920c5e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Remove one variable at a time from the LE linear models, retrain on the useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a536d7a-8e1f-41bd-ac43-4d2ce5d158af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model_remove(model_name, month_, region_list, lag_list, \n",
    "                              start_end_yr, n_epoch, learn_rate, \n",
    "                              extra_drop_=None, white_noise_=None):\n",
    "    '''\n",
    "    Train the 4 machine learning models for a given large ensemble on a \n",
    "    specificed of sea ice concentration data for a specified set of months,\n",
    "    regions, and lags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str,\n",
    "        The name of the GCM which provides a sufficiently large ensemble.\n",
    "    month_: int,\n",
    "        The months of sea ice concentration anomalies on which to train the \n",
    "        machine learning model. \n",
    "    region_list: list of ints,\n",
    "        A list of the regional sea ice anomalies to train the model on \n",
    "        separately.\n",
    "    lag_list: list of ints,\n",
    "        The range of lagged year on which to separately train the machine \n",
    "        learning model \n",
    "    start_end_yr: list, length 2 with integers,\n",
    "        The start and end years (inclusive) for the sea ice concentration data.\n",
    "    extra_drop: none or list of strings,\n",
    "        If False, no additonal variables are dropped. If a list of a string or\n",
    "        strings, those variables listed will be removed from the CVDP data. \n",
    "    white_noise: bool,\n",
    "        If True, add a variable of normalized random values in a gaussian \n",
    "        distribution for the 4 seasons, all members and all years.\n",
    "    n_epoch: int, \n",
    "        number of epochs with which to train all of the machine learning \n",
    "        models.\n",
    "    learn_rate: int,\n",
    "        Integer corresponding to the learning rate.         \n",
    "                      \n",
    "    Returns\n",
    "    ----------\n",
    "    r_values_xr: xarray.Dataset,\n",
    "        The pearson correlation coefficients for the linear model\n",
    "        for the validation data.\n",
    "    all_1_grads_xr: xarray.Dataset,\n",
    "        The gradients from the machine learning model using a simple multiple\n",
    "        linear regression model.    \n",
    "    '''\n",
    "    if white_noise_:\n",
    "        fewer_var = 0\n",
    "    else:\n",
    "        fewer_var = 1\n",
    "    \n",
    "    if type(extra_drop_) != type(None):\n",
    "        n_features = int((15-len(extra_drop_)-fewer_var)*4)\n",
    "    else:\n",
    "        n_features = int((15-fewer_var)*4)\n",
    "    \n",
    "    if model_name == 'CMIP6':\n",
    "        doi_model = '10.5194/gmd-9-1937-2016'\n",
    "    else:\n",
    "        doi_model = CMIP6_info['doi'].sel(model=model_name).values\n",
    "    \n",
    "    all_1_weights = np.empty([len(region_list), len(lag_list), n_features], \n",
    "                             dtype=float)\n",
    "    \n",
    "    all_r_values = np.empty([len(region_list), len(lag_list)], dtype=float)\n",
    "\n",
    "    for region_i, region_ in enumerate(region_list):\n",
    "        \n",
    "        for lag_i, lag_ in enumerate(lag_list):\n",
    "            \n",
    "            #load the feature and target data for the correct model, month,\n",
    "            #region and lag\n",
    "            CVDP_train, CVDP_valid, CVDP_test = load_CVDP(\n",
    "                model_name, month_, lag_, start_end_yr, \n",
    "                extra_drop=extra_drop_, white_noise=white_noise_\n",
    "            )\n",
    "            \n",
    "            target_train, target_valid, target_test = load_SIC(\n",
    "                model_name, month_, region_, start_end_yr, \n",
    "            )\n",
    "\n",
    "            ML_model_computed = []\n",
    "            ML_model_use = nn.Sequential(nn.Linear(n_features,1,bias=False))\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                params=ML_model_use.parameters(), \n",
    "                lr=learn_rate\n",
    "            )\n",
    "\n",
    "            ##### train the model #####\n",
    "            train_r, valid_r = [],[]\n",
    "            for epoch in range(n_epoch):\n",
    "                # TRAIN\n",
    "                prediction = ML_model_use(CVDP_train)\n",
    "                optimizer.zero_grad() #reset the gradients to zeros\n",
    "                loss = loss_fcn(prediction[:,0].double(), target_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_r.append(\n",
    "                    np.corrcoef(prediction[:,0].detach().numpy(),\n",
    "                                target_train.detach().numpy()\n",
    "                               )[1][0]\n",
    "                )\n",
    "\n",
    "                ML_model_computed.append(ML_model_use)\n",
    "\n",
    "                ##### validate the model #####\n",
    "                with torch.no_grad():\n",
    "                    p_val = ML_model_use(CVDP_valid)\n",
    "                    loss_val = loss_fcn(p_val[:,0].double(), target_valid)\n",
    "                    valid_r.append(np.corrcoef(p_val[:,0].detach().numpy(),\n",
    "                                    target_valid.detach().numpy())[1][0])\n",
    "\n",
    "                    #select highest validation r value from all epochs\n",
    "                    all_r_values[region_i][lag_i] = np.max(valid_r)\n",
    "\n",
    "                    all_1_weights[region_i][lag_i] = np.ravel(\n",
    "                        ML_model_computed[np.argmax(valid_r)][                               \n",
    "                            0].weight[0,:].detach().numpy())\n",
    "\n",
    "    if model_name == 'CMIP6':\n",
    "        model_name = 'CMIP6 multi-model large ensemble'\n",
    "        \n",
    "    if (white_noise_ == None) and (extra_drop_ == None):\n",
    "        mode_month_list = [\n",
    "            item for item in var_month_list if 'RAND' not in item]\n",
    "    elif (len(extra_drop_) == 1) and (white_noise_ == True):\n",
    "        mode_month_list = [\n",
    "            item for item in var_month_list if extra_drop_[0] not in item]\n",
    "    else:\n",
    "        extra_drop_inc_rand = np.append(extra_drop_, 'RAND')\n",
    "        mode_month_list = var_month_list.copy()\n",
    "        for i in extra_drop_inc_rand:\n",
    "            mode_month_list = [\n",
    "                item for item in mode_month_list if i not in item]\n",
    "\n",
    "    \n",
    "    #save the r values to NetCDF every model and month\n",
    "    r_values_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'r_value':(['region', 'lag'], all_r_values)},\n",
    "        coords = {\n",
    "            'region':region_list, 'lag':lag_list},\n",
    "    )\n",
    "\n",
    "    #save the linear 1 layer neural network weights to NetCDF\n",
    "    all_1_weights_xr = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'weights':(['region', 'lag', 'mode_month'], all_1_weights),\n",
    "        },\n",
    "        coords = {'region':region_list, 'lag':lag_list, \n",
    "                  'mode_month':mode_month_list,\n",
    "        },\n",
    "    )\n",
    "        \n",
    "    return(r_values_xr, all_1_weights_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab3ea2-abeb-40da-a746-bb3902044a88",
   "metadata": {},
   "source": [
    "## Remove one variable at a time from the LE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a017572a-8b14-456b-9353-930d00bfa1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the following dictionaries to determine which months and regions to \n",
    "#retrain the model on. Based on best 5 year lag r2 above persistence. \n",
    "\n",
    "#use the months from the best CMIP6 values\n",
    "good_GCM_months = {\n",
    "    '1': [9, 9, 9, 9, 9, 9],\n",
    "    '2': [8, 8, 8, 8, 8, 8, 8],\n",
    "    '3': [10, 10, 10, 10],\n",
    "    '4': [10, 10],\n",
    "    '5': [10, 10],\n",
    "    '6': [9, 9, 9],\n",
    "    '11':[8, 8, 8]\n",
    "}\n",
    "\n",
    "good_GCM_names = {\n",
    "    '1': ['CanESM5', 'CESM2-LENS', 'MIROC6', 'GISS-E2-1-G', 'IPSL-CM6A-LR',\n",
    "          'GISS-E2-1-H'],\n",
    "    '2': ['CanESM5', 'MIROC6', 'GISS-E2-1-G', 'ACCESS-ESM1-5', 'IPSL-CM6A-LR',\n",
    "          'MIROC-ES2L', 'UKESM1-0-LL'],\n",
    "    '3': ['CanESM5', 'MIROC6', 'IPSL-CM6A-LR', 'MIROC-ES2L'],\n",
    "    '4': ['CanESM5', 'UKESM1-0-LL'],\n",
    "    '5': ['CanESM5', 'UKESM1-0-LL'],\n",
    "    '6': ['CanESM5', 'GISS-E2-1-H', 'UKESM1-0-LL'],\n",
    "    '11':['CanESM5', 'ACCESS-ESM1-5', 'UKESM1-0-LL']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae8440b8-ec6e-4e03-bd6d-938e2735bb73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "\n",
    "var_list_use = list(CVDP_sample.to_array()['variable'].drop_sel(\n",
    "    variable=['AMOC','NINO12','NINO3','NINO4']).values)+['RAND']\n",
    "\n",
    "\n",
    "for region_key in [1,2,3,4,5,6,11]:\n",
    "    print(datetime.datetime.now(), region_key)\n",
    "    \n",
    "    for GCM_i, GCM in enumerate(good_GCM_names[str(region_key)]):\n",
    "        print(datetime.datetime.now(), GCM)\n",
    "        \n",
    "        doi_model = CMIP6_info['doi'].sel(model=GCM).values\n",
    "\n",
    "        month__ = good_GCM_months[str(region_key)][GCM_i]\n",
    "        \n",
    "        #skip this one if already run this analysis\n",
    "        if len(glob.glob(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'validation_r_values_linear_{GCM}_month_{str(month__).zfill(2)}_'\\\n",
    "            +f'var_15_drop_1_region_{region_key}.nc')):\n",
    "            continue\n",
    "\n",
    "        r_values_list = []\n",
    "        weights_list  = []\n",
    "        for drop_var in var_list_use:\n",
    "\n",
    "            if drop_var == 'RAND':\n",
    "                r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                    model_name=GCM, month_=month__, region_list=[region_key], \n",
    "                    lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                    n_epoch=2000, learn_rate=5e-4, extra_drop_=None,\n",
    "                    white_noise_=None,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                    model_name=GCM, month_=month__, region_list=[region_key], \n",
    "                    lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                    n_epoch=2000, learn_rate=5e-4, extra_drop_=[drop_var],\n",
    "                    white_noise_=True,\n",
    "                )  \n",
    "\n",
    "            r_values_list.append(r_values_xr)\n",
    "            weights_list.append(all_weights_xr)\n",
    "\n",
    "        r_values_save = xr.concat((r_values_list), dim='drop_var')\n",
    "        r_values_save['drop_var'] = var_list_use\n",
    "\n",
    "        weights_save = xr.concat((weights_list), dim='drop_var')\n",
    "        weights_save['drop_var'] = var_list_use\n",
    "\n",
    "        r_values_attrs = {\n",
    "            'Description': 'Pearson correlation coefficient for validation '\\\n",
    "                +f'data (15%) of members from the global climate model {GCM}. '\\\n",
    "                +'14 of the 15 modes '\\\n",
    "                +'of variability as computed by the Climate Variability '\\\n",
    "                +'Diagnostics Package (CVDP) with 2 year lowpass filtered '\\\n",
    "                +'sea ice concentration. These climate modes are as follows: '\\\n",
    "                +'AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, NPO, PNA, '\\\n",
    "                +'NAO, SAM, TAS, RAND. The linear model is trained at '\\\n",
    "                +'different lag times of 1-20 years and for each '\\\n",
    "                +'region (regions are defined by NSIDC MASIE-NH Version 1 '\\\n",
    "                +'(doi:10.7265/N5GT5K3K). Using an L1 loss function and Adam '\\\n",
    "                +'optimizer with 2000 epochs: nn.Sequential(nn.Linear(56,1,'\\\n",
    "                +'bias=False)), learning rate = 5e-4.',\n",
    "            'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "                \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "            'Data source': f'CMIP6 global climate model {GCM}, doi:'\\\n",
    "                +f'{doi_model} for historical sea ice concentration '\\\n",
    "                +'simulation, climate modes calculated by CVDP '\\\n",
    "                +'(doi:10.1002/2014EO490002)',\n",
    "            'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "                +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "        }\n",
    "\n",
    "        r_values_save.attrs = r_values_attrs\n",
    "        r_values_save.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'validation_r_values_linear_{GCM}_month_{str(month__).zfill(2)}_'\\\n",
    "            +f'var_15_drop_1_region_{region_key}.nc'\n",
    "        )\n",
    "\n",
    "\n",
    "        weights_attrs = r_values_attrs.copy()\n",
    "        weights_attrs['Description'] = 'Linear weights including 14 of the '\\\n",
    "            +'15 modes of variability as computed by the Climate Variability '\\\n",
    "            +'Diagnostics Package (CVDP) with 2 year lowpass filtered sea ice '\\\n",
    "            +'concentration. The linear model is trained/validated on the '\\\n",
    "            +'first 75/15% of the members from the global climate model '\\\n",
    "            +f'{GCM}. The climate modes are as follows: AMO, IPO, NINO34, '\\\n",
    "            +'PDO, AMM, ATN, IOD, NPI, NAM, NPO, PNA, NAO, SAM, TAS, RAND. '\\\n",
    "            +'The linear model is trained at different lag times of 1-20 '\\\n",
    "            +'years and for each region (regions are defined by NSIDC '\\\n",
    "            +'MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). Using an L1 loss '\\\n",
    "            +'function and Adam optimizer with 2000 epochs: '\\\n",
    "            +'nn.Sequential(nn.Linear(56,1,bias=False)), learning rate = 5e-4.'\n",
    "\n",
    "        weights_save.attrs = weights_attrs\n",
    "        weights_save.to_netcdf(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'weights_linear_{GCM}_month_{str(month__).zfill(2)}_'\\\n",
    "            +f'var_15_drop_1_region_{region_key}.nc'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e2d5d-2c33-49bc-8d1c-94b23c25e644",
   "metadata": {},
   "source": [
    "### Remove all variables with a worse effect on skill than a random variable for the LEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6a1e45-ca8a-4d71-acef-9c1a2c9cb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all of the drop1 LE data\n",
    "r_vals_LE_drop1 = []\n",
    "\n",
    "for region_i, region_ in enumerate([1,2,3,4,5,6,11]):\n",
    "    \n",
    "    region_r_vals = []\n",
    "    for GCM_i, GCM in enumerate(good_GCM_names[str(region_)]):\n",
    "        month_ = good_GCM_months[str(region_)][GCM_i]\n",
    "\n",
    "        r_values = xr.open_dataset(\n",
    "            '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "            +f'validation_r_values_linear_{GCM}_month_{str(month_).zfill(2)}'\\\n",
    "            +f'_var_15_drop_1_region_{region_}.nc'\n",
    "        )\n",
    "        \n",
    "        region_r_vals.append(r_values['r_value'])\n",
    "    \n",
    "    region_r_vals = xr.concat((region_r_vals), dim='model_name')\n",
    "    region_r_vals['model_name'] = good_GCM_names[str(region_)]    \n",
    "    r_vals_LE_drop1.append(region_r_vals)\n",
    "    \n",
    "r_vals_LE_drop1 = xr.concat((r_vals_LE_drop1), dim='region')\n",
    "r_vals_LE_drop1['region'] = [1,2,3,4,5,6,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf65945-a3e8-4fb1-a68c-9dd504e1a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "lag_range = np.arange(5,11)\n",
    "\n",
    "month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "               'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "for region_key in [1,2,3,4,5,6,11]:\n",
    "    print(datetime.datetime.now(), region_key)\n",
    "    \n",
    "    GCM_r_vals = []\n",
    "    GCM_weights = []\n",
    "    drop_dict = {}\n",
    "    for GCM_i, GCM in enumerate(good_GCM_names[str(region_key)]):\n",
    "        \n",
    "        doi_model = CMIP6_info['doi'].sel(model=GCM).values\n",
    "\n",
    "        month__ = good_GCM_months[str(region_key)][GCM_i]\n",
    "        \n",
    "        r_diff = r_vals_LE_drop1.sel(region=region_key).sel(lag=lag_range).mean(\n",
    "            'lag').sel(model_name=GCM)**2 - r_vals_LE_drop1.sel(\n",
    "            region=region_key).sel(lag=lag_range).mean('lag').sel(\n",
    "            model_name=GCM).sel(drop_var='RAND')**2\n",
    "\n",
    "        drop_list = r_diff['drop_var'].where(r_diff>0, drop=True).values\n",
    "        drop_dict[GCM] = drop_list\n",
    "        print(datetime.datetime.now(), GCM, drop_list)\n",
    "        \n",
    "        r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "            model_name=GCM, month_=month__, region_list=[region_key], \n",
    "            lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "            n_epoch=2000, learn_rate=5e-4, extra_drop_=drop_list,\n",
    "            white_noise_=False,\n",
    "        )  \n",
    "        \n",
    "        GCM_r_vals.append(r_values_xr)\n",
    "        GCM_weights.append(all_weights_xr)\n",
    "        \n",
    "    region_r_vals = xr.concat((GCM_r_vals), dim='model_name')\n",
    "    region_r_vals['model_name'] = good_GCM_names[str(region_key)]\n",
    "    \n",
    "    region_weights = xr.concat((GCM_weights), dim='model_name')\n",
    "    region_weights['model_name'] = good_GCM_names[str(region_key)]\n",
    "\n",
    "    r_values_attrs = {\n",
    "        'Description': 'Pearson correlation coefficient for validation '\\\n",
    "            +'data (15%) of members from the global climate models as '\\\n",
    "            +f'follows {good_GCM_names[str(region_key)]} for '\\\n",
    "            +f'{month_names[month__-1]}, with modes of '\\\n",
    "            +f'variability yielding lower predictive skill than a random '\\\n",
    "            +f'variable dropped as follows: {drop_dict}. The full list of 15 '\\\n",
    "            +'variables is: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "            +'NPO, PNA, NAO, SAM, TAS, as computed by the Climate Variability '\\\n",
    "            +'Diagnostics Package (CVDP) with 2 year lowpass filtered '\\\n",
    "            +'sea ice concentration. The linear model is trained at '\\\n",
    "            +f'different lag times of 1-20 years, the region is {region_key} '\\\n",
    "            +'as defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "            +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "            +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "            +'learning rate = 5e-4.',\n",
    "        'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "            \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "        'Data source': f'CMIP6 global climate model historical simulations '\\\n",
    "            +'for historical sea ice concentration, with climate modes '\\\n",
    "            +'calculated by CVDP (doi:10.1002/2014EO490002)',\n",
    "        'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "            +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "    }\n",
    "\n",
    "    region_r_vals.attrs = r_values_attrs\n",
    "    region_r_vals.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_linear_LEs_region_{region_key}_month_{month__}'\\\n",
    "        +f'dropped_useless_vars.nc'\n",
    "    )\n",
    "\n",
    "\n",
    "    weights_attrs = r_values_attrs.copy()\n",
    "    weights_attrs['Description'] = 'Linear weights of the useful modes of '\\\n",
    "        +'variability as computed by the Climate Variability Diagnoistics'\\\n",
    "        +'Package (CVDP) with 2 year lowpass filtered sea ice concentration '\\\n",
    "        +f'for {month_names[month__-1]}. The linear '\\\n",
    "        +f'model is trained/validated on the first 75/15% of the members '\\\n",
    "        +f'from the global climate models {good_GCM_names[str(region_key)]}. '\\\n",
    "        +'Climate modes which do not yield a higher predictive skill than a '\\\n",
    "        +f'random variable are removed as follows {drop_dict}. The full list '\\\n",
    "        +'of variables is as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, '\\\n",
    "        +'NPI, NAM, NPO, PNA, NAO, SAM, TAS. The linear model is trained at '\\\n",
    "        +f'different lag times of 1-20 years and for region {region_key} as'\\\n",
    "        +'defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "        +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "        +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "        +'learning rate = 5e-4.'\n",
    "\n",
    "    region_weights.attrs = weights_attrs\n",
    "    region_weights.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_LEs_region_{region_key}_month_{month__}_'\\\n",
    "        +f'dropped_useless_vars.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4880a8-b81c-4c0d-9edb-584bb11df100",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Remove one variable at a time from the CMIP6 model, and retrain on the useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "342d543c-fd3b-45cc-99af-2577c601d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "\n",
    "var_list_use = list(CVDP_sample.to_array()['variable'].drop_sel(\n",
    "        variable=['AMOC','NINO12','NINO3','NINO4']).values)+['RAND']\n",
    "\n",
    "\n",
    "for month__ in np.arange(1,13):\n",
    "    print(datetime.datetime.now(), month__)\n",
    "    r_values_list = []\n",
    "    weights_list  = []\n",
    "    \n",
    "    for drop_var in var_list_use:\n",
    "        print(datetime.datetime.now(), drop_var)\n",
    "        \n",
    "        if drop_var == 'RAND':\n",
    "            r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                'CMIP6', month_=month__, region_list=[1,2,3,4,5,6,11], \n",
    "                lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                n_epoch=2000, learn_rate=5e-4, extra_drop_=None,\n",
    "                white_noise_=None,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "                'CMIP6', month_=month__, region_list=[1,2,3,4,5,6,11], \n",
    "                lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "                n_epoch=2000, learn_rate=5e-4, extra_drop_=drop_var,\n",
    "                white_noise_=True,\n",
    "            )    \n",
    "\n",
    "        r_values_list.append(r_values_xr)\n",
    "        weights_list.append(all_weights_xr)\n",
    "        \n",
    "    r_values_save = xr.concat((r_values_list), dim='drop_var')\n",
    "    r_values_save['drop_var'] = var_list_use\n",
    "    \n",
    "    weights_save = xr.concat((weights_list), dim='drop_var')\n",
    "    weights_save['drop_var'] = var_list_use\n",
    "\n",
    "    r_values_attrs = {\n",
    "        'Description': 'Pearson correlation coefficient for validation '\\\n",
    "            +'data, 2nd member from 41 CMIP6 GCMs. Training data was the '\\\n",
    "            +'first member. 14 of the 15 modes of variability as computed by '\\\n",
    "            +'the Climate Variability Diagnostics Package (CVDP) with 2 year '\\\n",
    "            +'lowpass filtered sea ice concentration. These climate modes are '\\\n",
    "            +'as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "            +'NPO, PNA, NAO, SAM, TAS, RAND. The linear model is trained '\\\n",
    "            +'at different lag times of 1-20 years and for each '\\\n",
    "            +'region (regions are defined by NSIDC MASIE-NH Version 1 '\\\n",
    "            +'(doi:10.7265/N5GT5K3K). Using an L1 loss function and Adam '\\\n",
    "            +'optimizer with 2000 epochs: nn.Sequential(nn.Linear(56,1,'\\\n",
    "            +'bias=False)), learning rate = 5e-4.',\n",
    "        'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "            \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "        'Data source': f'CMIP6 global climate model multi-model ensemble'\\\n",
    "            +f'doi:10.5194/gmd-9-1937-2016 for historical sea ice '\\\n",
    "            +'concentration simulation, climate modes calculated by CVDP '\\\n",
    "            +'(doi:10.1002/2014EO490002)',\n",
    "        'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "            +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "    }\n",
    "\n",
    "    r_values_save.attrs = r_values_attrs\n",
    "    r_values_save.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_linear_CMIP6_month_{str(month__).zfill(2)}_'\\\n",
    "        +'var_15_drop_1.nc'\n",
    "    )\n",
    "\n",
    "\n",
    "    weights_attrs = r_values_attrs.copy()\n",
    "    weights_attrs['Description'] = 'Linear weights including 14 of the 15 '\\\n",
    "        +'modes of variability as computed by the Climate Variability '\\\n",
    "        +'Diagnostics Package (CVDP) with 2 year lowpass filtered sea ice '\\\n",
    "        +'concentration. The linear model is trained on the 1st member of all '\\\n",
    "        +'CMIP6 GCMs and validated on the second members. The climate modes '\\\n",
    "        +'are as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "        +'NPO, PNA, NAO, SAM, TAS, RAND. The linear model is trained at '\\\n",
    "        +'different lag times of 1-20 years and for each region (regions are '\\\n",
    "        +'defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). Using '\\\n",
    "        +'an L1 loss function and Adam optimizer with 2000 epochs: '\\\n",
    "        +'nn.Sequential(nn.Linear(56,1,bias=False)), learning rate = 5e-4.'\n",
    "\n",
    "    weights_save.attrs = weights_attrs\n",
    "    weights_save.to_netcdf(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'weights_linear_CMIP6_month_{str(month__).zfill(2)}_'\\\n",
    "        +'var_15_drop_1.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679849b-9b2e-471b-bf6a-9b2561be4971",
   "metadata": {},
   "source": [
    "## Retrain the CMIP6 linear model with only the useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "875dc7cd-1eee-434d-834c-c0c203eadb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all of the drop1 CMIP6 data\n",
    "r_vals_CMIP6_drop1 = []\n",
    "for month_ in np.arange(1,13):\n",
    "\n",
    "    r_values = xr.open_dataset(\n",
    "        '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "        +f'validation_r_values_linear_CMIP6_month_{str(month_).zfill(2)}'\\\n",
    "        +f'_var_15_drop_1.nc'\n",
    "    )\n",
    "\n",
    "    r_vals_CMIP6_drop1.append(r_values['r_value'])\n",
    "    \n",
    "r_vals_CMIP6_drop1 = xr.concat((r_vals_CMIP6_drop1), dim='month')\n",
    "r_vals_CMIP6_drop1['month'] = np.arange(1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8c5f76c-652e-45df-af7d-40daa5f78faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#obtain all of the train/validate/test GCM members\n",
    "CMIP6_GCM_list = []\n",
    "for GCM in np.sort(list(good_GCM_mem.keys())):    \n",
    "    n_mem = len(good_GCM_mem[GCM])\n",
    "    if n_mem > 2:\n",
    "        CMIP6_GCM_list.append(GCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f7d24dc-6bf6-4700-84e1-4a84468a42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn  = torch.nn.L1Loss() #keep all models sparse\n",
    "lag_range = np.arange(5,11)\n",
    "\n",
    "month_names = ['January', 'February', 'March', 'April', 'May', 'June', 'July', \n",
    "               'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "#select the months where multi-model ensemble has highest r2 above persistence\n",
    "good_CMIP6_months = [9, 8, 10, 10, 10, 9, 8]\n",
    "\n",
    "CMIP6_r_vals  = []\n",
    "CMIP6_weights = []\n",
    "for region_i, region_key in enumerate([1,2,3,4,5,6,11]):\n",
    "    print(datetime.datetime.now(), region_key)\n",
    "\n",
    "    month__ = good_CMIP6_months[region_i]\n",
    "\n",
    "    r_diff = r_vals_CMIP6_drop1.sel(region=region_key).sel(month=month__).sel(\n",
    "        lag=lag_range).mean('lag')**2 - r_vals_CMIP6_drop1.sel(\n",
    "        region=region_key).sel(month=month__).sel(lag=lag_range).mean(\n",
    "        'lag').sel(drop_var='RAND')**2\n",
    "\n",
    "    drop_list = r_diff['drop_var'].where(r_diff>0, drop=True).values\n",
    "    drop_dict[str(region_)] = drop_list\n",
    "    print(datetime.datetime.now(), drop_list)\n",
    "\n",
    "    r_values_xr, all_weights_xr = train_linear_model_remove(\n",
    "        model_name='CMIP6', month_=month__, region_list=[region_key], \n",
    "        lag_list=np.arange(1,21), start_end_yr=[1941,2014], \n",
    "        n_epoch=2000, learn_rate=5e-4, extra_drop_=drop_list,\n",
    "        white_noise_=False,\n",
    "    )  \n",
    "\n",
    "    CMIP6_r_vals.append(r_values_xr)\n",
    "    CMIP6_weights.append(all_weights_xr)\n",
    "        \n",
    "region_r_vals = xr.concat((CMIP6_r_vals), dim='region')\n",
    "region_r_vals['region'] = [1,2,3,4,5,6,11]\n",
    "\n",
    "region_weights = xr.concat((CMIP6_weights), dim='region')\n",
    "region_weights['region'] = [1,2,3,4,5,6,11]\n",
    "\n",
    "r_values_attrs = {\n",
    "    'Description': 'Pearson correlation coefficient for validation '\\\n",
    "        +'data from all second ensemble members from 42 CMIP6 Global Climate '\\\n",
    "        +f'Models, train on first members. The full list of GMCs is as '\\\n",
    "        +f'follows: {CMIP6_GCM_list}, for regions [1,2,3,4,5,6,11] with '\\\n",
    "        +f'months corresponding to {good_GCM_months}. The modes of '\\\n",
    "        +f'variability yielding lower predictive skill than a random variable '\\\n",
    "        +f'are dropped as follows: {drop_dict}. The full list of 15 '\\\n",
    "        +'variables is: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, NPI, NAM, '\\\n",
    "        +'NPO, PNA, NAO, SAM, TAS, as computed by the Climate Variability '\\\n",
    "        +'Diagnostics Package (CVDP) with 2 year lowpass filtered '\\\n",
    "        +'sea ice concentration. The linear model is trained at '\\\n",
    "        +f'different lag times of 1-20 years, the regions are defined '\\\n",
    "        +'by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "        +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "        +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "        +'learning rate = 5e-4.',\n",
    "    'Timestamp'  : str(datetime.datetime.utcnow().strftime(\n",
    "        \"%H:%M UTC %a %Y-%m-%d\")),\n",
    "    'Data source': f'CMIP6 global climate model historical simulations '\\\n",
    "        +'for historical sea ice concentration, with climate modes '\\\n",
    "        +'calculated by CVDP (doi:10.1002/2014EO490002)',\n",
    "    'Analysis'   : 'https://github.com/chrisrwp/low-frequency-'\\\n",
    "        +'variability/blob/main/neural_network/Train_4_ML_Models.ipynb',\n",
    "}\n",
    "\n",
    "region_r_vals.attrs = r_values_attrs\n",
    "region_r_vals.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    +f'validation_r_values_linear_CMIP6_all_regions_best_months_'\\\n",
    "    +f'dropped_useless_vars.nc'\n",
    ")\n",
    "\n",
    "\n",
    "weights_attrs = r_values_attrs.copy()\n",
    "weights_attrs['Description'] = 'Linear weights of the useful modes of '\\\n",
    "    +'variability as computed by the Climate Variability Diagnoistics'\\\n",
    "    +'Package (CVDP) with 2 year lowpass filtered sea ice concentration '\\\n",
    "    +f'for all regions [1,2,3,4,5,6,11] for months {good_GCM_months}. '\\\n",
    "    +f'The linear model is trained/validated on the first/second of all '\\\n",
    "    +f'members from the global climate models as follows {CMIP6_GCM_list}. '\\\n",
    "    +'Climate modes which do not yield a higher predictive skill than a '\\\n",
    "    +f'random variable are removed as follows {drop_dict}. The full list '\\\n",
    "    +'of variables is as follows: AMO, IPO, NINO34, PDO, AMM, ATN, IOD, '\\\n",
    "    +'NPI, NAM, NPO, PNA, NAO, SAM, TAS. The linear model is trained at '\\\n",
    "    +f'different lag times of 1-20 years and for region {region_key} as'\\\n",
    "    +'defined by NSIDC MASIE-NH Version 1 (doi:10.7265/N5GT5K3K). '\\\n",
    "    +'Training uses an L1 loss function and Adam optimizer with 2000 '\\\n",
    "    +'epochs: nn.Sequential(nn.Linear(n_features,1,bias=False)), '\\\n",
    "    +'learning rate = 5e-4.'\n",
    "\n",
    "region_weights.attrs = weights_attrs\n",
    "region_weights.to_netcdf(\n",
    "    '/glade/work/cwpowell/low-frequency-variability/PyTorch_models/'\\\n",
    "    +f'weights_linear_CMIP6_all_regions_best_months_'\\\n",
    "    +f'dropped_useless_vars.nc'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL-3.7.9",
   "language": "python",
   "name": "npl-3.7.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
